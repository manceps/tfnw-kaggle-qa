{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERTjoint yes no2.1.ipynb","provenance":[{"file_id":"1EJoauf9c5eqXSQMSBd9znN0iMtRc4g0z","timestamp":1576555737603}],"collapsed_sections":["lRKAhZK8yvO_","Vk7GFxxMNKej"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"906bfcecf750402c81e00a5a3e849191":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_23390be1425546789971d760dc12b605","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_910688a37f75413c96ff0b322e8352f8","IPY_MODEL_09f616deb510420885275d15298c8a9f"]}},"23390be1425546789971d760dc12b605":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"910688a37f75413c96ff0b322e8352f8":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9020a00282674cfab4ddd2af4f2ed110","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":100,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":100,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bf1e882f4a004b6faccb9569fc18851f"}},"09f616deb510420885275d15298c8a9f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_63aed3dcbf04486db71bfd2729d8852c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 100/100 [00:00&lt;00:00, 120.20it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ce6ccf9738dd4097b902154b19fe0258"}},"9020a00282674cfab4ddd2af4f2ed110":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bf1e882f4a004b6faccb9569fc18851f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"63aed3dcbf04486db71bfd2729d8852c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ce6ccf9738dd4097b902154b19fe0258":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"Js7aj8RDhSjz","colab_type":"text"},"source":["# BERTjoint Question Answering Contest"]},{"cell_type":"markdown","metadata":{"id":"SA1MAb09BlQB","colab_type":"text"},"source":["I prefer to do my primary development in a Colab virtual machine but, competitions require your kernel to run on Kaggle for scoring. You can start with this notebook and add your own project code which should run in either location if you use the directory variables for file locations and correctly configure data and user libraries in Kaggle.<p>\n","The main differences are the file locations. On Colab you will symlink to a google Drive and for Kaggle you will need to upload your files as a .zip data file If you want SSH access there is code at bottom to allow this for Colab (not possible on Kaggle).<p>\n","If you run into problems or have suggestions I'd love to hear from you."]},{"cell_type":"markdown","metadata":{"id":"93AfEfEABMXB","colab_type":"text"},"source":["## Explanation"]},{"cell_type":"markdown","metadata":{"id":"eKO_K9DlBuyk","colab_type":"text"},"source":["### Data & Directories\n","There are a couple of differences between Kaggle and Colab.<p>\n","**Colab** - Is a live linux system with nothing being persistant. You can attach a google drive to your kernel and/or download files from gs://, GitHub, Kaggle, etc. Using drive can have a performance penalty but is the easiest way to access persistent files.<br>\n","I am symlinking Library files and output files.<br>\n","Because my google drive space is limited I choose to download large data files each time the kernel is used.<p>\n","**Kaggle** - The kernel has a persistent ./input directory for data that is read only. You can attach data and library files there. (zip your files into a single file and upload the .zip).<br>\n","Your private ./lib directory can ba zipped and uploaded there as ./input/lib<br>\n","You can also create notebooks of kernel type \"script\" and then file->Add Utility Script in your competition notebook to attach the script files under ./usr/lib but you have to do this one file at a time. (see: https://www.kaggle.com/product-feedback/91185 for more information)<br>\n","I don't think competition scoring allows internet access so all files have to be attached to your notebook at submit time."]},{"cell_type":"markdown","metadata":{"id":"gS2HFoX_qHN8","colab_type":"text"},"source":["### SSH\n","You can SSH into the Colab. If you do this you can vim the ./lib files directly and they will be\n","persistant. Also, since all project and notebook ./lib files are in gdrive you can access all of them through any of your Colab kernels by using the gdrive directory."]},{"cell_type":"markdown","metadata":{"id":"IOQz9QFOB5EB","colab_type":"text"},"source":["### Switching between Colab & Kaggle\n","One way of moving your script from Colab to Kaggle to run is:<br>\n","   * delete all cells from your Kaggle competition notebook<br>\n","   * download the .ipynb from Colab<br>\n","   * upload it into the blank Kaggle notebook.<br>\n","   * delete the cells near the bottom of the notebook that need to be deleted when running on Kaggle<br>\n","   * update any script parameters (eg. verbose)\n","   * zip your current library files into a lib.zip and upload to your Kaggle dataset file\n","   * if you have changed ./data files make sure your Kaggle kernel has the current versions of those files"]},{"cell_type":"markdown","metadata":{"id":"7dMWt8F8B9k2","colab_type":"text"},"source":["### Directory Structure (Notebook)\n","\n","* Kaggle &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <em>(cwd = /kaggle/working/)</em><br>\n","  {datadir} = /kaggle/input<br>\n","  {libdir} = /kaggle/input/lib &nbsp; &nbsp; &nbsp; (or /kaggle/usr/lib)<br>\n","  {outdir} = /kaggle/working<br>\n","* Colab &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <em>(cwd = /content/)</em><br>\n","  {datadir} = /content/data<br>\n","  {libdir} = /content/lib<br>\n","  {outdir} = /content/output<br>\n","\n","#### - Required Libraries\n","   * {libdir}/bert_utils.py\n","   * {libdir}/modeling.py\n","   * {libdir}/tokenization.py\n","\n","#### - Inputs (competition data)\n","   * {datadir}/tensorflow2-question-answering/ (from https://www.kaggle.com/c/tensorflow2-question-answering/data)\n","\n","#### - Required Data (additional packages)\n","   * {datadir}/bert-joint-baseline/ (from prvi\n","https://www.kaggle.com/prokaj/bert-joint-baseline; contains model and scripts)\n","\n","#### - Outputs\n","   * {outdir}/predictions.json\n","   * {outdir}/submission.csv<br>\n","   * {outdir}/eval.tf_record<br>\n","   * {outdir}/.ipynb_checkpoints/<br>"]},{"cell_type":"markdown","metadata":{"id":"ftZ8SR0WC5Nj","colab_type":"text"},"source":["### Drectory Structure (Google Drive)\n","The following is the file structure for your google drive:<p>\n","\n","/My Drive/Colab/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <em>(this directory should be private)</em><br>\n","/My Drive/Colab/kaggle.json &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <em>(your personal Kaggle auth file)</em><br>\n","/My Drive/Colab/{projdir}/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (folder for a specific project/competition)<br>\n","/My Drive/Colab/{projdir}/data/&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(optional source for project data)<br>\n","/My Drive/Colab/{projdir}/{notebook}/&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(individual notebook within project)<br>\n","/My Drive/Colab/{projdir}/{notebook}/notebook.ipynb<br>\n","/My Drive/Colab/{projdir}/{notebook}/lib/<br>\n","/My Drive/Colab/{projdir}/{notebook}/output/<br>\n","/My Drive/{projdir}/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <em>(optional directory to be shared between teams)</em><br><p>\n","Using this directory structure and the variables in the config variables section below you can have any number of <br>projects on your google drive and any number of notebook versions in each project.<br> Each notebook can have a unique set of libraries.<p>\n","There is also a {nbver} which, if not '' is appended to the end of {outdir} and {libdir} offering additional flexibility <br>if you want. If not just set it to '' (empty string)\n"]},{"cell_type":"markdown","metadata":{"id":"NxmuhjhzpZqL","colab_type":"text"},"source":["### GitHub\n","As I learn more about GitHub I will add information here. But, I think you can choose to make repositaries out of either /My Drive/Colab/{projdir}/ or /My Drive/Colab/{projdir}/{notebook}/   <p>\n","You can then either pull/push to the shared repo directly from the Colab (using ssh, using %%bash commands in notebook or by linking the google Drive to your local machine and doing it from there.<p>\n","From within the Colab all files are in your google drive from the path /content/gdrive/My\\ Drive/Colab/..."]},{"cell_type":"markdown","metadata":{"id":"tiQT_p1OCN9J","colab_type":"text"},"source":["### - Notes\n","Put notes about your Notebook here"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jCycAU1aL99h"},"source":["\n","\n","\n","\n","\n","\n","### - Credits / Ancestry\n","If your notebook is a fork or combination of other notebooks here you should provide links so other people can look at where you built your current work from.<p>\n","This notebook is a fork of [mmmarchetti's notebook](https://www.kaggle.com/mmmarchetti/tensorflow-2-0-bert-yes-no-answers) which was a fork of [prokaj's - bert joint baseline notebook](https://www.kaggle.com/prokaj/bert-joint-baseline-notebook/notebook).<br>\n","mmmarchetti made some modifications to slightly improve the code and get the YES / NO answers and leave the unknowns blank."]},{"cell_type":"markdown","metadata":{"id":"jNoEkLc0CXXu","colab_type":"text"},"source":["# ========== Notebook Variables =========="]},{"cell_type":"code","metadata":{"id":"pUu4nppvoEEb","colab_type":"code","colab":{}},"source":["CompSubmission = False                       # Set to True if submitting to Competition"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNmKNoxqqARP","colab_type":"code","colab":{}},"source":["## Turn this on for development to make sure library updates get reimported\n","#  will reduce performance so comment out for production.\n","%load_ext autoreload\n","%autoreload 2\n","\n","from IPython.core.debugger import set_trace      ## allow set_trace()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HaUs7DxSKDVG","colab_type":"code","colab":{}},"source":["## Helper Functions   (probably will move to a library eventually)\n","import os\n","\n","# Custom Error Handler\n","class ExecutionStop(Exception):\n","    ''' forces notebook to stop with a raised exception '''\n","    def __init__(self, value): self.value=value\n","    def __str__(self): return(str(self.value))\n","\n","#  Show list of file sand directories                 (it seems that this is skipping the symlinks)\n","def list_files(startpath, exclude=[\"/.config\", \"/gdrive\", \"/bertqa\", \"__pycache__\"]):\n","    ''' Lists files in {startpath} optionally excluding {exclude} directories '''\n","    for root, _, files in os.walk(startpath, followlinks=True):\n","        if any([e in root for e in exclude]):\n","            continue\n","        level = root.replace(startpath, '').count(os.sep)\n","        indent = ' ' * 4 * (level)\n","        print(f\"{indent}{os.path.basename(root)}/\")\n","        subindent = ' ' * 4 * (level + 1)\n","        for f in files:\n","            print(f\"{subindent}{f}\")\n","\n","def print_flags():\n","    ''' Prints the program config flags '''\n","    if verbose:\n","        print(\"\\nParameters:\")\n","        FLAGS = tf.flags.FLAGS\n","        for attr, obj in sorted(FLAGS.__flags.items()):\n","            print(f\"{attr.upper()}={obj.value}\")\n","        print(\"\")\n","\n","def del_all_flags(FLAGS):\n","    ''' Deletes all program flags '''\n","    flags_dict = FLAGS._flags()\n","    keys_list = [keys for keys in flags_dict]\n","    for keys in keys_list:\n","        FLAGS.__delattr__(keys)\n","\n","# raise ExecutionStop(\"Message\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F30YID4j_khp","colab_type":"code","outputId":"4d59ac4e-074b-4438-c554-9189d4b21ba6","executionInfo":{"status":"ok","timestamp":1579003519026,"user_tz":480,"elapsed":2600,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["## Set file locations    (these variables are not implemented in the FLAGS code yet)\n","import os, sys\n","from pathlib import Path\n","\n","## Config Variables\n","verbose = True if not CompSubmission else False  # Turn this off to supress some of the \"fyi\" output\n","competition = 'tensorflow2-question-answering'\n","train_file = ''                       # Set this below when you download the training file\n","test_file = ''                        # Set this below when you download the test file\n","projdir = 'bertqa'                    # The project directory on Drive for this competition\n","notebook = 'BERTjoint_yes_no'         # Subdir on Drive for files specific to this notebook\n","nbver = ''                            # library/output subfolder for this notebook version, or ''\n","DownloadBigFiles = True               # Files will not download if already on drive\n","EnableSMSMessages = False             # Allows notebook to attempt to send SMS alerts\n","\n","if Path('/content').exists():\n","    print(\"Detected running on Colab\")\n","    kernel = 'Colab'\n","    basedir = '/content'\n","    libdir = f\"{basedir}/lib\"\n","    datadir = f\"{basedir}/data\"\n","    outdir = f\"{basedir}/output\"      # will be symlinked to a user's private gdrive for persistence\n","elif Path('/kaggle').exists():\n","    print(\"Detected running on Kaggle\")\n","    kernel = 'Kaggle'\n","    basedir = '/kaggle'\n","    libdir = f\"{basedir}/input/BERTjoint_yes_no_lib2\"   # you will upload a {name}.zip file as a dataset\n","    datadir = f\"{basedir}/input\"      # this may need to be '../input' for scoring\n","    outdir = f\"{basedir}/working\"     # this may need to be '.' for scoring\n","else:\n","    raise ExecutionStop(\"Cannot continue without determining file locations\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Detected running on Colab\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MNzAuLZDzG_R","colab_type":"text"},"source":["# ============= Machine Spinup ============="]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fYw0W98nXsD0","outputId":"0469b9fb-33d6-44af-ae9c-bf670b07707b","executionInfo":{"status":"ok","timestamp":1579003521504,"user_tz":480,"elapsed":5068,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":166}},"source":["! zdump PST\n","if verbose:\n","    list_files(basedir)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["PST  Tue Jan 14 12:05:19 2020 PST\n","content/\n","    sample_data/\n","        anscombe.json\n","        README.md\n","        california_housing_test.csv\n","        california_housing_train.csv\n","        mnist_train_small.csv\n","        mnist_test.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P4DT47HCDmwM","colab_type":"code","colab":{}},"source":["if kernel == \"Colab\":\n","    if Path(f\"{basedir}/sample_data\").exists():\n","        ! rm -rf \"{basedir}/sample_data\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aheaRzVE6fs1","colab_type":"text"},"source":["## -- Setup --"]},{"cell_type":"markdown","metadata":{"id":"gixU6_gv678n","colab_type":"text"},"source":["### Google Drive"]},{"cell_type":"markdown","metadata":{"id":"s02B0uj8DbPr","colab_type":"text"},"source":["<font color=\"pink\">Note: I am using an abreviated section of the train_file as the {test_file} so we have answers to eval.<br>\n","Whenever you change the source for {test_file} you have to delete the nq-test.tfrecords file and remake the<br>\n","sample_submission.csv file (see code block after Google Drive)"]},{"cell_type":"code","metadata":{"id":"auXx45x70Qcs","colab_type":"code","outputId":"1aeefe73-d5c9-458a-9030-e801715eb4a6","executionInfo":{"status":"ok","timestamp":1579003638729,"user_tz":480,"elapsed":122273,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":836}},"source":["## File link to Google Drive\n","\n","######\n","##  WARNING: This is convienent but a rather bad idea from a security point of view. Mounting your\n","##           Google Drive in this way makes your entire drive accessible read/write and then you\n","##           are likely to run code from libraries that could be untrustable.\n","##           Possible alternatives would be to use read only file sharing links or if you want\n","##           read/write access (to allow file output for example) I would recommend creating a\n","##           special Google Drive account that only has files related to your Colab work.\n","######\n","\n","if kernel == 'Colab':\n","    from google.colab import drive\n","    drive.mount(f\"{basedir}/gdrive\", force_remount=False)   # true to reread drive\n","\n","    # this is a link to our team shared google folder (no longer sure this is useful)\n","    # Create a shorter shared directory name and avoid having to deal with the space in \"My Drive\"\n","    if Path(f\"{basedir}/{projdir}\").is_symlink():\n","        ! rm \"{basedir}/{projdir}\"\n","    ! ln -s \"{basedir}/gdrive/My Drive/{projdir}/\" \"{basedir}/{projdir}\"\n","    if not Path(f\"{basedir}/{projdir}\").exists():\n","        raise ExecutionStop(\"Symlink to shared project directory not found!\")\n","\n","    ## If you do not want to use the lib directoy from your Google Drive set this block False\n","    if True:\n","        if Path(libdir).is_symlink():\n","            ! rm \"{libdir}\"\n","        ! ln -s \"{basedir}/gdrive/My Drive/Colab/{projdir}/{notebook}/lib{nbver}/\" \"{libdir}\"\n","        if not Path(libdir).exists():\n","            raise ExecutionStop(\"Project libdir directory not found!\")\n","\n","    ## If you do not want output to be written to your Google Drive set this block False\n","    if True:\n","        if Path(outdir).is_symlink():\n","            ! rm \"{outdir}\"\n","        ! ln -s \"{basedir}/gdrive/My Drive/Colab/{projdir}/{notebook}/output{nbver}/\" \"{outdir}\"\n","        if not Path(outdir).exists():\n","            raise ExecutionStop(\"Project outdir directory not found!\")\n","\n","    ## If you want to use data from gDrive set to True, if False this will download data\n","    #  Reading off google drive appears to be 8 times slower so rather than mapping we are\n","    #  copying so you only pay the price once\n","    if True:\n","        altdatasrc = f\"{basedir}/gdrive/My Drive/Colab/{projdir}/data\"\n","        if not Path(\"{altdatasrc}/compdata.flag\"):\n","            raise ExecutionStop(\"Could not find compdata on Google Drive!\")\n","        if not Path(f\"{datadir}/compdata.flag\").exists():      ## Don't do anything if flag exists\n","            print(\"\\nGetting Competition Data From Google Drive\")\n","            ! [ -d \"{datadir}/{competition}\" ] || mkdir -p \"{datadir}/{competition}\"\n","            ! cp \"{altdatasrc}/{competition}/sample_submission.csv\" \"{datadir}/{competition}/\"\n","            ! cp \"{altdatasrc}/{competition}/simplified-nq-train.jsonl\" \"{datadir}/{competition}\"\n","            ! cp \"{altdatasrc}/{competition}/simplified-nq-test.jsonl\" \"{datadir}/{competition}\"\n","            ! cp \"{altdatasrc}/{competition}/simplified-nq-eval.jsonl\" \"{datadir}/{competition}\"\n","            ! cp \"{altdatasrc}/{competition}\"/*.tfrecords \"{datadir}/{competition}\"\n","            ! touch \"{datadir}/compdata.flag\"\n","            train_file = f\"{datadir}/{competition}/simplified-nq-train.jsonl\"\n","            test_file = f\"{datadir}/{competition}/simplified-nq-eval.jsonl\"\n","        if not Path(\"{altdatasrc}/bertdata.flag\"):\n","            raise ExecutionStop(\"Could not find bertfiles on Google Drive!\")\n","        if not Path(f\"{datadir}/bertfiles.flag\").exists():      ## Don't do anything if flag exists\n","            print(\"Getting BERTjoint Model From Google Drive\")\n","            ! [ -d \"{datadir}/bert-joint-baseline\" ] || mkdir -p \"{datadir}/bert-joint-baseline\"\n","            ! cp \"{altdatasrc}/bert-joint-baseline\"/* \"{datadir}/bert-joint-baseline/\"\n","            ! touch \"{datadir}/bertfiles.flag\"\n","\n","    if verbose:\n","        print(f\"\\nCWD: {basedir}\\n\")\n","        ! ls -lh \"{basedir}\"\n","        print()\n","        list_files(basedir)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","\n","Getting Competition Data From Google Drive\n","Getting BERTjoint Model From Google Drive\n","\n","CWD: /content\n","\n","total 12K\n","lrwxrwxrwx 1 root root   32 Jan 14 12:05 bertqa -> '/content/gdrive/My Drive/bertqa/'\n","drwxr-xr-x 4 root root 4.0K Jan 14 12:06 data\n","drwx------ 4 root root 4.0K Jan 14 12:05 gdrive\n","lrwxrwxrwx 1 root root   59 Jan 14 12:05 lib -> '/content/gdrive/My Drive/Colab/bertqa/BERTjoint_yes_no/lib/'\n","lrwxrwxrwx 1 root root   62 Jan 14 12:05 output -> '/content/gdrive/My Drive/Colab/bertqa/BERTjoint_yes_no/output/'\n","\n","content/\n","    data/\n","        compdata.flag\n","        bertfiles.flag\n","        bert-joint-baseline/\n","            bert_config.json\n","            model_cpkt-1.data-00000-of-00002\n","            model_cpkt-1.data-00001-of-00002\n","            bert-model-prokaj.flag\n","            vocab-nq.txt\n","            model_cpkt-1.index\n","        tensorflow2-question-answering/\n","            nq-test.tfrecords\n","            sample_submission.csv\n","            simplified-nq-test.jsonl\n","            simplified-nq-eval.jsonl\n","            simplified-nq-train.jsonl\n","    lib/\n","        modeling.py\n","        tokenization.py\n","        flag_defaults.py\n","        bert_utils.py\n","        natural_questions/\n","            nq_eval.py\n","            eval_utils.py\n","    output/\n","        submission.csv\n","        predictions.json\n","        model.png\n","        .ipynb_checkpoints/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V_meD4QbEJ3c","colab_type":"code","colab":{}},"source":["## Rewrite the sample_submission.csv file. You only need to do this if you have changed the \n","#  data in {file_test}. If you have done that you need to recreate this file and copy it back to\n","#  whereever you are copying it to the VM from.\n","\n","if False:\n","    import csv\n","    ! rm \"{datadir}/{competition}/sample_submission.csv\"\n","    with open(f\"{datadir}/{competition}/sample_submission.csv\", 'w') as file:\n","        writer = csv.writer(file)\n","        writer.writerow([\"example_id\",\"PredictionString\"])\n","\n","        ## examples seems to be lists of len 1 containing an bert_utils.NQ_Example\n","        for examples in bert_utils.nq_examples_iter( input_file = f\"{test_file}\", \n","                                                    is_training = True,     # whether to include answer\n","                                                    tqdm = tqdm_notebook):\n","            ## uncomment this line to see answers\n","            # print(examples[0].example_id, examples[0].answer)\n","            writer.writerow([f\"{examples[0].example_id}_long\", ''])\n","            writer.writerow([f\"{examples[0].example_id}_short\", ''])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fvKSjUFEVJ6i","colab_type":"text"},"source":["### SMS Messaging\n","<Details>You will need an account with http://twillio.com for this to work and have copied your auth API token tnto a file on your google drive.<p>\n","You will also need a json file with the following config information:<p>\n","```\n","{\n","\"account_sid\":\"{your account_sid}\",\n","\"auth_token\":\"{your auth_token}\",\n","\"send_to\":\"+1{delivery number}\",\n","\"send_from\":\"+1{your twilio number}\"\n","}\n","```\n","</Details>"]},{"cell_type":"code","metadata":{"id":"JbdR3jpaVdVY","colab_type":"code","colab":{}},"source":["if EnableSMSMessages and kernel == 'Colab':\n","    ! pip3 install twilio > /dev/null\n","    from twilio.rest import Client as TC\n","    import json\n","    twil_file = f\"{basedir}/gdrive/My Drive/Colab/twilio.json\" \n","    if Path(twil_file).exists():\n","        # if there is a twilio.json file in gdrive use it\n","        with open(twil_file, 'r') as f:\n","            twil_auth = json.load(f)\n","\n","        account_sid = twil_auth[\"account_sid\"]\n","        auth_token  = twil_auth[\"auth_token\"]\n","\n","        sms = TC(account_sid, auth_token)\n","    else:\n","        sms = None\n","else:\n","    sms = None\n","\n","def sms_message(str_msg):\n","    ''' Sends messages through sms gateway to notify user of alerts.\n","        If not enabled calls to sms_message() will silently do nothing.\n","        For this to work you need an account at https://www.twilio.com/\n","        Config through a twilio.json file (called above) that must contains\n","        {account_sid, auth_token, send_to, send_from}\n","    '''\n","    if sms is not None:\n","        message = sms.messages.create(\n","            to=twil_auth['send_to'], \n","            from_=twil_auth['send_from'],\n","            body=str_msg)\n","        if message.error_message is not None:\n","            print(f\"\\nSMS Error: {message.error_message}\")\n","\n","sms_message(\"Twilio SMS gateway from Colab enabled\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8BLJAW95ih-","colab_type":"text"},"source":["### Kaggle API\n","<Details>You will need Kaggle API token to link the Colab instance to your Kaggle account to get data, etc.<br>\n","Go to: https://www.kaggle.com/yourID/account and click on the \"Create New API Token: button to get a file named kaggle.json.<p>You can put your kaggle.json file in your google drive at My Drive/colab/kaggle.json.<br>\n","Alternately, you can store it on your local machine and the script will ask you to upload it.</Details>"]},{"cell_type":"code","metadata":{"id":"lc3IYPUg17c8","colab_type":"code","outputId":"bb266ac9-494b-47fd-ae47-c9634d13eb60","executionInfo":{"status":"ok","timestamp":1579003640764,"user_tz":480,"elapsed":124279,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["## Link to Kaggle\n","if kernel == 'Colab':\n","    from google.colab import files\n","    if Path(f\"{basedir}/gdrive/My Drive/Colab/kaggle.json\").exists():\n","        # if there is a kaggle.json file in gdrive use it\n","        os.environ['KAGGLE_CONFIG_DIR'] = f\"{basedir}/gdrive/My Drive/Colab/\"\n","        ! ls -l \"{basedir}/gdrive/My Drive/Colab/kaggle.json\"\n","    else:\n","        # Have user upload file\n","        print('Upload kaggle.json.')\n","        # The files.upload() command is failing sporatically with:\n","        #   TypeError: Cannot read property '_uploadFiles' of undefined (just run this cell again)\n","        ! rm \"{basedir}/kaggle.json\"  2> /dev/null\n","        files.upload()\n","        ! chmod 600 kaggle.json\n","        os.environ['KAGGLE_CONFIG_DIR'] = f\"{basedir}/\"\n","        ! ls -l \"{basedir}/kaggle.json\"\n","    # import kaggle                           # gives us access to kaggle.api\n","    # help(kaggle)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["-rw------- 1 root root 66 Dec 15 08:31 '/content/gdrive/My Drive/Colab/kaggle.json'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M0k60Je4YEQa","colab_type":"text"},"source":["## -- Main System Config --\n","<Details><Summary>Global Config</Summary>\n","Put any global system configuration here"]},{"cell_type":"code","metadata":{"id":"JNKoTizrh9RG","colab_type":"code","colab":{}},"source":["%%bash -s \"{libdir}\" \"{datadir}\" \"{outdir}\"\n","# make directories if not already exist\n","[ -d \"$1\" ] || mkdir -p \"$1\"        # {libdir}\n","[ -d \"$2\" ] || mkdir -p \"$2\"        # {datadir}\n","[ -d \"$3\" ] || mkdir -p \"$3\"        # {outdir}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UdjitRnQynwk","colab_type":"code","colab":{}},"source":["import sys, os\n","if not libdir in sys.path:                       # don't add multiple times\n","    sys.path.append(libdir)\n","if not (libdir in os.environ['PYTHONPATH']):     # needed to run python scripts from shell\n","    os.environ['PYTHONPATH'] += f\":{libdir}\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Pq9DBlECVVvY","outputId":"72234237-723e-4e1e-cd4f-aec067a1ba34","executionInfo":{"status":"ok","timestamp":1579003643490,"user_tz":480,"elapsed":126976,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":300}},"source":["if verbose:\n","    print('', \"sys.path:\", *sys.path, '', sep='\\n')\n","    !printenv |grep -E 'KAGGLE|PYTHON'\n","    print()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["\n","sys.path:\n","\n","/env/python\n","/usr/lib/python36.zip\n","/usr/lib/python3.6\n","/usr/lib/python3.6/lib-dynload\n","/usr/local/lib/python3.6/dist-packages\n","/usr/lib/python3/dist-packages\n","/usr/local/lib/python3.6/dist-packages/IPython/extensions\n","/root/.ipython\n","/content/lib\n","\n","KAGGLE_CONFIG_DIR=/content/gdrive/My Drive/Colab/\n","PYTHONWARNINGS=ignore:::pip._internal.cli.base_command\n","PYTHONPATH=/env/python:/content/lib\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"javRwDS6zhOC","colab_type":"text"},"source":["# =========== Project Setup Stuff ==========="]},{"cell_type":"markdown","metadata":{"id":"gAqCNh4y8DJz","colab_type":"text"},"source":["### Download Dataset and Support Files"]},{"cell_type":"markdown","metadata":{"id":"cGGM0EVmM24H","colab_type":"text"},"source":["Kaggle Competition Files"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5XcpPkyLBzaA","outputId":"bb880881-6b9c-480c-e60f-3060864de9b3","executionInfo":{"status":"ok","timestamp":1579003645675,"user_tz":480,"elapsed":129151,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":133}},"source":["## Competition Dataset\n","if DownloadBigFiles and kernel == 'Colab':\n","    if not Path(f\"{datadir}/compdata.flag\").exists():      ## Don't download again if exists\n","        # comp data might exist from previous run or because you copied it from gDrive\n","        print(\"Downloading Competition Data\\n\")\n","        ! kaggle competitions download -c \"{competition}\" -p \"{datadir}\"\n","        ! mkdir -p \"{datadir}/{competition}/\"\n","        ! mv \"{datadir}/sample_submission.csv\"  \"{datadir}/{competition}\"\n","        ! unzip \"{datadir}/simplified-nq-train.jsonl.zip\" -d \"{datadir}/{competition}\"\n","        ! rm \"{datadir}/simplified-nq-train.jsonl.zip\"\n","        ! unzip \"{datadir}/simplified-nq-test.jsonl.zip\" -d \"{datadir}/{competition}\"\n","        ! rm \"{datadir}/simplified-nq-test.jsonl.zip\"\n","        ! touch \"{datadir}/compdata.flag\"\n","        train_file = f\"{datadir}/{competition}/simplified-nq-train.jsonl\"\n","        test_file = f\"{datadir}/{competition}/simplified-nq-test.jsonl\"\n","    else:\n","        print(\"Competition Data already exists. Not downloading.\\n\")\n","        !ls -lh \"{datadir}/{competition}\"/*\n","else:\n","    print(\"For Kaggle, make sure you download a copy of the competition data into your kernel\")\n","    ! ls -lh \"{datadir}/{competition}\"/*\n","    train_file = f\"{datadir}/{competition}/simplified-nq-train.jsonl\"\n","    test_file = f\"{datadir}/{competition}/simplified-nq-test.jsonl\"\n","\n","    # public_dataset = os.path.getsize(f\"{test_file}\") < 20_000_000\n","    private_dataset = os.path.getsize(f\"{test_file}\") >= 20_000_000"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Competition Data already exists. Not downloading.\n","\n","-rw------- 1 root root 7.9M Jan 14 12:06 /content/data/tensorflow2-question-answering/nq-test.tfrecords\n","-rw------- 1 root root 5.5K Jan 14 12:05 /content/data/tensorflow2-question-answering/sample_submission.csv\n","-rw------- 1 root root 4.4M Jan 14 12:06 /content/data/tensorflow2-question-answering/simplified-nq-eval.jsonl\n","-rw------- 1 root root 477K Jan 14 12:06 /content/data/tensorflow2-question-answering/simplified-nq-test.jsonl\n","-rw------- 1 root root  53M Jan 14 12:06 /content/data/tensorflow2-question-answering/simplified-nq-train.jsonl\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oZhNV3PJHOEL","colab_type":"text"},"source":["BERTjoint files from: https://www.kaggle.com/prokaj/bert-joint-baseline<p>\n","<font color='pink'>We probably want to use a different source if possible because this archive has a 1GB .tfrecords file we are not currently using so slower than it should be. For now I am using a smaller copy on my Google Drive</font>\n"]},{"cell_type":"code","metadata":{"id":"s207mPqAHDVw","colab_type":"code","outputId":"e9e807b6-8694-438c-e1c5-b018da960fe7","executionInfo":{"status":"ok","timestamp":1579003647977,"user_tz":480,"elapsed":131444,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":166}},"source":["# Get BERTjoint model files (this a copy of the prokaj file from my Google Drive)\n","if DownloadBigFiles and kernel == 'Colab':\n","    if not Path(f\"{datadir}/bertfiles.flag\").exists():      ## Don't download again if exists\n","        print(\"Downloading BERT-joint Model\\n\")\n","        ! mkdir -p \"{datadir}/bert-joint-baseline/\"\n","        filestoget = \"bert_config* model_cpkt* nq-test* vocab*\"\n","        ! kaggle datasets download -d prokaj/bert-joint-baseline -p \"{datadir}\"\n","        ! unzip \"{datadir}/bert-joint-baseline.zip\" {filestoget} -d \"{datadir}/bert-joint-baseline/\"\n","        ! rm \"{datadir}/bert-joint-baseline.zip\"\n","        if Path(f\"{datadir}/bert-joint-baseline-output.npz\").exists():\n","            ! rm \"{datadir}/bert-joint-baseline-output.npz\" # if kaggle downloaded this delete it\n","        if verbose:\n","            ! ls -lh \"{datadir}/bert-joint-baseline/\"\n","        ! touch \"{datadir}/bertfiles.flag\"\n","    else:\n","        print(\"BERT-joint Files already exists. Not downloading.\\n\")\n","        ! ls -lh \"{datadir}/bert-joint-baseline/\"\n","else:\n","    print(\"For Kaggle, make sure you download a copy of prokaj's bert-joint-baseline to your kernel\")\n","    ! ls -lh \"{datadir}/bert-joint-baseline/\"\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["BERT-joint Files already exists. Not downloading.\n","\n","total 1.3G\n","-rw------- 1 root root  314 Jan 14 12:06 bert_config.json\n","-rw------- 1 root root    0 Jan 14 12:06 bert-model-prokaj.flag\n","-rw------- 1 root root  78K Jan 14 12:06 model_cpkt-1.data-00000-of-00002\n","-rw------- 1 root root 1.3G Jan 14 12:06 model_cpkt-1.data-00001-of-00002\n","-rw------- 1 root root  29K Jan 14 12:06 model_cpkt-1.index\n","-rw------- 1 root root 227K Jan 14 12:06 vocab-nq.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XJoB8YB3MrvX","colab_type":"text"},"source":["BERTjoint files from: \n","https://github.com/google-research/language/tree/master/language/question_answering/bert_joint\n"]},{"cell_type":"code","metadata":{"id":"1uvuFGJ0MqHo","colab_type":"code","colab":{}},"source":["# Get BERTjoint model files    (we are not using this model / files)\n","if False and DownloadBigFiles and kernel == 'Colab':\n","    if not Path(f\"{datadir}/bertfiles.flag\").exists():\n","        print(\"Downloading BERT-joint Model\\n\")\n","        ! gsutil cp -R gs://bert-nq/bert-joint-baseline \"{datadir}\"\"\n","        ! touch \"{datadir}/bertfiles.flag\"\n","    else:\n","        print(\"BERT-joint Files already exists. Not downloading.\\n\")\n","        ! ls -lh \"{datadir}/bert-joint-baseline/\"\n","else:\n","    pass        # if Kaggle, data will already be there"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c4NrOOtpOVN0","colab_type":"text"},"source":["BERT files from: https://github.com/google-research/bert<br>\n","(Not the model we are using at the moment)"]},{"cell_type":"code","metadata":{"id":"Pdp6tqXmmLw_","colab_type":"code","colab":{}},"source":["## get BERT (this is unlikely to be the BERT-joint files needed for competition)\n","# this version of BERT seems won't import as is. On line 88 of lib/bert/optimization.py\n","#    change   tr.train.Optimizer to tf.keras.optimizers.Optimizer\n","if False and DownloadBigFiles and kernel == 'Colab':\n","    ! git clone https://github.com/google-research/bert.git\n","    ! mv bert lib\n","\n","    # get some pretrained models  (I really  have no idea what these are or if useful)\n","    ! wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n","    ! unzip cased_L-12_H-768_A-12.zip\n","    ! rm cased_L-12_H-768_A-12.zip"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bNPxSmhMJC5c"},"source":["### Library Setup"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"T1jkYjB4XsDk"},"source":["Custom libraries are uploaded to google Drive and symlinked if on Colab and zipped and<br>\n","uploaded to a dataset if on Kaggle."]},{"cell_type":"code","metadata":{"id":"yr69yF8fepTA","colab_type":"code","cellView":"both","outputId":"9a8d4ced-a67f-4e10-eed8-0887ca2b6e17","executionInfo":{"status":"ok","timestamp":1579003650234,"user_tz":480,"elapsed":133652,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":316}},"source":["# if you need to move/copy any lib files to final locations do it here\n","if kernel == 'Colab':\n","    pass\n","if kernel == 'Kaggle':\n","    ! cp \"{datadir}/bert-joint-baseline\"/*.py \"{libdir}\"\n","\n","if verbose:\n","    print(f\"{libdir}/\")\n","    ! ls -lh \"{libdir}\"/*"],"execution_count":18,"outputs":[{"output_type":"stream","text":["/content/lib/\n","-rw------- 1 root root  43K Jan 14 08:07 /content/lib/bert_utils.py\n","-rw------- 1 root root 7.5K Jan  7 06:53 /content/lib/flag_defaults.py\n","-rw------- 1 root root  41K Jan  5 00:36 /content/lib/modeling.py\n","-rw------- 1 root root  13K Jan  5 00:36 /content/lib/tokenization.py\n","\n","/content/lib/natural_questions:\n","total 36K\n","-rw------- 1 root root  15K Jan 14 11:31 eval_utils.py\n","-rw------- 1 root root  17K Jan 14 11:25 nq_eval.py\n","drwx------ 2 root root 4.0K Jan 13 08:24 __pycache__\n","\n","/content/lib/__pycache__:\n","total 72K\n","-rw------- 1 root root  25K Jan 14 08:07 bert_utils.cpython-36.pyc\n","-rw------- 1 root root 5.1K Jan 13 08:24 flag_defaults.cpython-36.pyc\n","-rw------- 1 root root  31K Jan 13 08:23 modeling.cpython-36.pyc\n","-rw------- 1 root root 9.7K Jan 13 08:23 tokenization.cpython-36.pyc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qhpBRp11R0jv","colab_type":"code","outputId":"a07cb553-62d8-415d-87e9-77bb1a24b51b","executionInfo":{"status":"ok","timestamp":1579003660206,"user_tz":480,"elapsed":143608,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["## Load Libraries\n","import os, sys, importlib\n","\n","if kernel == \"Colab\":               # Kaggle is V2 by default\n","    #magic to make Colab path to Tensorflow V2 on Colab\n","    %tensorflow_version 2.x \n","\n","import tensorflow as tf\n","print(\"TensorFlow\", tf.__version__)\n","\n","import numpy as np\n","import pandas as pd\n","import collections\n","\n","import bert_utils\n","import modeling\n","import tokenization\n","\n","import json"],"execution_count":19,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n","TensorFlow 2.1.0-rc1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9rqDriKq8g52","colab":{}},"source":["# raise ExecutionStop(\"Execution stopped\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l1FG1uHaqwIz","colab_type":"text"},"source":["# =========== Project Code ==========="]},{"cell_type":"markdown","metadata":{"id":"NsnKczo07ygW","colab_type":"text"},"source":["## -- Code Implementation in Tensorflow 2.0 --"]},{"cell_type":"markdown","metadata":{"id":"v7JSYsp-72La","colab_type":"text"},"source":["**A few notes:**\n","- Since we won't use it with the kernels, he removed most of the **TPU** related stuff to reduce complexity.\n","- Tensorflow 2 don't let us use global variables **(tf.compat.v1.trainable_variables())**.\n","\n","In this notebook, we'll be using the Bert baseline for Tensorflow to create predictions for the Natural Questions test set. Note that this uses a model that has already been pre-trained - we're only doing inference here. A GPU is required, and this should take between 1-2 hours to run.\n","\n","The original script can be found [here](https://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py).<br>\n","The supporting modules were drawn from the [official Tensorflow model repository](https://github.com/tensorflow/models/tree/master/official).<br>\n","The bert-joint-baseline data is described [here](https://github.com/google-research/language/tree/master/language/question_answering/bert_joint)."]},{"cell_type":"code","metadata":{"id":"rhcWhldD17Q4","colab_type":"code","outputId":"9768d721-78bd-492d-baa4-a15e2f53254f","executionInfo":{"status":"ok","timestamp":1579003662840,"user_tz":480,"elapsed":146209,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["! zdump PST"],"execution_count":21,"outputs":[{"output_type":"stream","text":["PST  Tue Jan 14 12:07:40 2020 PST\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"beaF_fYhtUxn","colab_type":"text"},"source":["### Support Classes"]},{"cell_type":"code","metadata":{"id":"VikoZMVKGq3G","colab_type":"code","colab":{}},"source":["# ### Values in bert_config.json\n","#  = {\n","# 'attention_probs_dropout_prob':0.1,\n","# 'hidden_act':'gelu', # 'gelu',\n","# 'hidden_dropout_prob':0.1,\n","# 'hidden_size':1024,\n","# 'initializer_range':0.02,\n","# 'intermediate_size':4096,\n","# 'max_position_embeddings':512,\n","# 'num_attention_heads':16,\n","# 'num_hidden_layers':24,\n","# 'type_vocab_size':2,\n","# 'vocab_size':30522\n","# }"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RQ1M5DHfc3HK","colab_type":"text"},"source":["#### Note from ChrisM\n","The Bert-Joint model builds on Bert by adding some new layers to the original Bert model.  One layer is a 2 dimensional layer which is applied to the sequence hidden states and represents the start and end logits.  In other words, each sequence token gets two logits which are used to generate logit probabilities that the token is a start/end token.   The other layer will encode a 5 dimensional vector which is a one-hot encoding for the response type as encoded in the Bert-Joint paper ( ie, [no response, yes, no, short, long] ).  Both of these additional layers are created as instances of the class TDense.\n","\n","The mk_model function adds these new layers to the the Bert model to create the Bert-Joint model.  This is just the model that the Bert-Joint paper builds\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"X4hOx_86tTFI","colab_type":"code","colab":{}},"source":["class TDense(tf.keras.layers.Layer):\n","    def __init__(self,\n","                 output_size,\n","                 kernel_initializer=None,\n","                 bias_initializer=\"zeros\",\n","                **kwargs):\n","        super().__init__(**kwargs)\n","        self.output_size = output_size\n","        self.kernel_initializer = kernel_initializer\n","        self.bias_initializer = bias_initializer\n","\n","    def build(self,input_shape):\n","        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n","        if not (dtype.is_floating or dtype.is_complex):\n","          raise TypeError(\"Unable to build `TDense` layer with \"\n","                          \"non-floating point (and non-complex) \"\n","                          \"dtype %s\" % (dtype,))\n","        input_shape = tf.TensorShape(input_shape)\n","        if tf.compat.dimension_value(input_shape[-1]) is None:\n","          raise ValueError(\"The last dimension of the inputs to \"\n","                           \"`TDense` should be defined. \"\n","                           \"Found `None`.\")\n","        last_dim = tf.compat.dimension_value(input_shape[-1])\n","#       self.input_spec = tf.keras.layers.InputSpec(min_ndim=3, axes={-1: last_dim})    # original value <<< mmm still same >>>\n","        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})    # ChrisM changed\n","        self.kernel = self.add_weight(\n","            \"kernel\",\n","            shape=[self.output_size,last_dim],\n","            initializer=self.kernel_initializer,\n","            dtype=self.dtype,\n","            trainable=True)\n","        self.bias = self.add_weight(\n","            \"bias\",\n","            shape=[self.output_size],\n","            initializer=self.bias_initializer,\n","            dtype=self.dtype,\n","            trainable=True)\n","        super(TDense, self).build(input_shape)\n","\n","    def call(self,x):\n","        return tf.matmul(x,self.kernel,transpose_b=True)+self.bias\n","\n","\n","def mk_model(config):\n","    seq_len = config['max_position_embeddings']\n","    unique_id  = tf.keras.Input(shape=(1,),dtype=tf.int64,name='unique_id')\n","    input_ids   = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='input_ids')\n","    input_mask  = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='input_mask')\n","    segment_ids = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='segment_ids')\n","    BERT = modeling.BertModel(config=config,name='bert')\n","    pooled_output, sequence_output = BERT(input_word_ids=input_ids,\n","                                          input_mask=input_mask,\n","                                          input_type_ids=segment_ids)\n","    \n","    logits = TDense(2,name='logits')(sequence_output)\n","    start_logits,end_logits = tf.split(logits,axis=-1,num_or_size_splits= 2,name='split')\n","    start_logits = tf.squeeze(start_logits,axis=-1,name='start_squeeze')\n","    end_logits   = tf.squeeze(end_logits,  axis=-1,name='end_squeeze')\n","    \n","    ans_type      = TDense(5,name='ans_type')(pooled_output)\n","    return tf.keras.Model([input_ for input_ in [unique_id,input_ids,input_mask,segment_ids] \n","                           if input_ is not None],\n","                          [unique_id,start_logits,end_logits,ans_type],\n","                          name='bert-baseline')    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qEnip0KLtx--","colab_type":"text"},"source":["### Create Model"]},{"cell_type":"code","metadata":{"id":"HNUN2aCD0sG5","colab_type":"code","outputId":"e0391a9b-9789-4024-9b77-821df0e81a26","executionInfo":{"status":"ok","timestamp":1579003664737,"user_tz":480,"elapsed":148066,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":100}},"source":["print(\"\\nGPU Memory\\n\")\n","!nvidia-smi --query-gpu=utilization.memory,memory.total,memory.free,memory.used --format=csv"],"execution_count":24,"outputs":[{"output_type":"stream","text":["\n","GPU Memory\n","\n","utilization.memory [%], memory.total [MiB], memory.free [MiB], memory.used [MiB]\n","0 %, 11441 MiB, 11441 MiB, 0 MiB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zTkY-he7qRbj","colab_type":"code","colab":{}},"source":["## grab bert config\n","with open(f\"{datadir}/bert-joint-baseline/bert_config.json\", 'r') as f:\n","    bert_config = json.load(f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xzVE2wQGt2Au","colab_type":"code","outputId":"9baa3caa-a3e4-4587-fd42-f8ced763f2e9","executionInfo":{"status":"ok","timestamp":1579003679892,"user_tz":480,"elapsed":163200,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":800}},"source":["if verbose:\n","    print(\"BERT config:\")\n","    model= mk_model(bert_config)\n","    print(json.dumps(bert_config, indent=4))\n","    print(\"\\nModel Summary\")\n","    model.summary()\n","    print()\n","    tf.keras.utils.plot_model( model,\n","                        to_file=f\"{outdir}/model.png\",    # Graphic output in outdir\n","                        show_shapes=True,\n","                        show_layer_names = True,\n","                        rankdir = 'TB',            # TB creates vertical; LR creates horizontal\n","                        expand_nested = True,     # expand nested models into clusters\n","                        dpi = 300\n","                        )"],"execution_count":26,"outputs":[{"output_type":"stream","text":["BERT config:\n","{\n","    \"attention_probs_dropout_prob\": 0.1,\n","    \"hidden_act\": \"gelu\",\n","    \"hidden_dropout_prob\": 0.1,\n","    \"hidden_size\": 1024,\n","    \"initializer_range\": 0.02,\n","    \"intermediate_size\": 4096,\n","    \"max_position_embeddings\": 512,\n","    \"num_attention_heads\": 16,\n","    \"num_hidden_layers\": 24,\n","    \"type_vocab_size\": 2,\n","    \"vocab_size\": 30522\n","}\n","\n","Model Summary\n","Model: \"bert-baseline\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_ids (InputLayer)          [(None, 512)]        0                                            \n","__________________________________________________________________________________________________\n","input_mask (InputLayer)         [(None, 512)]        0                                            \n","__________________________________________________________________________________________________\n","segment_ids (InputLayer)        [(None, 512)]        0                                            \n","__________________________________________________________________________________________________\n","bert (BertModel)                ((None, 1024), (None 335141888   input_ids[0][0]                  \n","                                                                 input_mask[0][0]                 \n","                                                                 segment_ids[0][0]                \n","__________________________________________________________________________________________________\n","logits (TDense)                 (None, 512, 2)       2050        bert[0][1]                       \n","__________________________________________________________________________________________________\n","tf_op_layer_split (TensorFlowOp [(None, 512, 1), (No 0           logits[0][0]                     \n","__________________________________________________________________________________________________\n","unique_id (InputLayer)          [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","tf_op_layer_start_squeeze (Tens [(None, 512)]        0           tf_op_layer_split[0][0]          \n","__________________________________________________________________________________________________\n","tf_op_layer_end_squeeze (Tensor [(None, 512)]        0           tf_op_layer_split[0][1]          \n","__________________________________________________________________________________________________\n","ans_type (TDense)               (None, 5)            5125        bert[0][0]                       \n","==================================================================================================\n","Total params: 335,149,063\n","Trainable params: 335,149,063\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JJOVtX_NuEA3","colab_type":"text"},"source":["### Checkpoint"]},{"cell_type":"code","metadata":{"id":"6eI_LdIAuGFV","colab_type":"code","outputId":"0120b052-6486-4553-befb-df89dc6ec37b","executionInfo":{"status":"ok","timestamp":1579003681147,"user_tz":480,"elapsed":164445,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["cpkt = tf.train.Checkpoint(model=model)\n","cpkt.restore(f\"{datadir}/bert-joint-baseline/model_cpkt-1\").assert_consumed()"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f2fdea4ec50>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"IDnBbvUCuHI-","colab_type":"text"},"source":["### Setting the Flags"]},{"cell_type":"code","metadata":{"id":"0D_JoNumuOPH","colab_type":"code","colab":{}},"source":["class DummyObject:\n","    def __init__(self,**kwargs):\n","        self.__dict__.update(kwargs)\n","\n","## 0.60 new code mmm    (ALC values in parens if different)\n","# Lots of stuff here but he only really uses the three values below\n","# FLAGS=DummyObject(skip_nested_contexts=True, #True\n","#                   max_position=50,            # set to same in bert_utils.py\n","#                   max_contexts=48,            # set to same in bert_utils.py\n","#                   max_query_length=64,        # set to same in bert_utils.py\n","#                   max_seq_length=512, #512      (384)    # set to same in bert_utils.py\n","#                   doc_stride=128,             # set to same in bert_utils.py\n","#                   include_unknowns=0.02, #0.02  (-1.0)   # changed from bert_utils.py\n","#                   n_best_size=5, #20            (20)     # changed from bert_utils.py\n","#                   max_answer_length=30, #30   # set to same in bert_utils.py\n","                  \n","#                   warmup_proportion=0.1,\n","#                   learning_rate=1e-5,   #       (5e-5)\n","#                   num_train_epochs=3.0,\n","#                   train_batch_size=32,\n","#                   num_train_steps=100000,   #   (None: features / batch_size * epochs)\n","#                   num_warmup_steps=10000,  #    (None: train_steps * warmup_proportion)\n","#                   max_eval_steps=100,     #     (Not persent)\n","#                   use_tpu=False,\n","#                   eval_batch_size=8,      #     (Not persent)\n","#                   max_predictions_per_seq=20) # (Not persent)\n","\n","# I treid changing n_best_size back to  20 and resubmitted. Still got 0.60\n","FLAGS=DummyObject(\n","                  max_seq_length=512, #512      (384)    # set to same in bert_utils.py\n","                  n_best_size=5, #20            (20)     # changed from bert_utils.py\n","                  max_answer_length=30  #30   # set to same in bert_utils.py\n","                 )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w9I5xtXYj5Gb","colab_type":"text"},"source":["#### Note from ChrisM\n","This next section does the processing from the json file to a tokenized representation.  The output from the nq_examples_iter iterator is an 'example' which holds a single question along with the candidate answers.  At the nq_examples_iter output level, tokenization to the Bert-Joint wordpiece level has not yet been performed.  **NOTE: the nq_examples_iter will by default skip over any candidate long answers that are subsets of other candidate long answers ( as indicated by the top_level=False indicator ).**\n","\n","Each 'example' contains the text of the question and an element called 'doc_tokens' which is a list of strings representing the long answer candidates all concatenated together:\n","*   The long answers have been split into whitespace delimited words\n","*   HTML characters have been removed\n","*   For each candidate long answer, the 'type' and 'position' of the answer in the text are recorded at the start in special tokens as in the Bert-Joint paper\n","*   'type' tells whether the long answer is a paragraph, table, list, or other as indicated by the first HTML character in the candidate answer\n","*   'position' indicates how many of the given answer type were seen previously in the document.  Ie, is the answer the first paragraph or the second, etc...\n","\n","After processing as above, the convert(example) line performs tokenization to the Bert-Joint wordpiece vocabulary and formats the candidate question-document pairs in the format required for Bert-Joint where you have the question first, then a SEP token, and then a fragment of the text where the text is stepped over in overlapping 128 token increments.  I am not sure, but I think in the original Bert-Joint, that they fed in the entire document but here we have been given a set of 'plausible' long answers and we only include those.  **Question:  do our long answer candidates in the dataset span the entire document? If not, would it be better to feed the whole document in?**\n","\n","After processing by convert(example), each json entry has been split into multiple InputFeatures objects which have the required tokenized input form for Bert-Joint as well as two identifiers:\n","*   example_index = the id from the json file\n","*   unique_id = example_index + 'a incrementing integer'\n","\n","The incrementing integer that contributes to unique_id is just an attempt to give unique ids to each question/document-fragment pair.  The unique_ids are, I think, not guaranteed to be unique unfortunately but they probably are.  It depends if any of the ids from the json file are close enough to each other.  We could test for this just to be sure.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"CSm1q0MouPwq","colab_type":"code","colab":{}},"source":["import tqdm\n","eval_records = f\"{datadir}/{competition}/nq-test.tfrecords\"\n","if kernel == 'Kaggle' and private_dataset:\n","    eval_records='nq-test.tfrecords'\n","if not Path(eval_records).exists():\n","    # tf2baseline.FLAGS.max_seq_length = 512\n","    eval_writer = bert_utils.FeatureWriter(\n","        filename=os.path.join(eval_records),\n","        is_training=False)\n","\n","    tokenizer = tokenization.FullTokenizer(vocab_file=f\"{datadir}/bert-joint-baseline/vocab-nq.txt\", \n","                                           do_lower_case=True)\n","\n","    features = []\n","    convert = bert_utils.ConvertExamples2Features(tokenizer=tokenizer,\n","                                                   is_training=False,\n","                                                   output_fn=eval_writer.process_feature,\n","                                                   collect_stat=False)\n","\n","    n_examples = 0\n","    tqdm_notebook= tqdm.tqdm_notebook if not kernel == 'Kaggle' else None\n","    for examples in bert_utils.nq_examples_iter(input_file=f\"{test_file}\", \n","                                           is_training=False,\n","                                           tqdm=tqdm_notebook):\n","        for example in examples:\n","            n_examples += convert(example)\n","\n","    eval_writer.close()\n","    print('number of test examples: %d, written to file: %d' % (n_examples,eval_writer.num_features))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dTJRYQjQuTsB","colab_type":"code","colab":{}},"source":["seq_length = FLAGS.max_seq_length       # bertconfig['max_position_embeddings']\n","name_to_features = {\n","      \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n","      \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n","      \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n","      \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n","  }\n","\n","def _decode_record(record, name_to_features=name_to_features):\n","    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n","    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n","\n","    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n","    # So cast all int64 to int32.\n","    for name in list(example.keys()):\n","        t = example[name]\n","        if name != 'unique_id': #t.dtype == tf.int64:\n","            # t = tf.cast(t, dtype=tf.int32)\n","            t = tf.cast(t, dtype=tf.int64)      ### new code mmm\n","        example[name] = t\n","\n","    return example\n","\n","def _decode_tokens(record):\n","    return tf.io.parse_single_example(serialized=record, \n","                                      features={\n","                                          \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n","                                          \"token_map\" :  tf.io.FixedLenFeature([seq_length], tf.int64)\n","                                      })\n","      \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKzXED-KuXjv","colab_type":"code","colab":{}},"source":["## Create ds which is a generator of input data to be fed into model.predict\n","raw_ds = tf.data.TFRecordDataset(eval_records)\n","token_map_ds = raw_ds.map(_decode_tokens)\n","decoded_ds = raw_ds.map(_decode_record)\n","ds = decoded_ds.batch(batch_size=32,drop_remainder=False)   ## a generator yielding batches of input samples"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b6dSZ_i53EgN","colab_type":"text"},"source":["After a few more cells above which just do some reformatting to a tensor-flow compatible structure,<br> we then run the next line which does the actual prediction<p>\n","Each ds item is a batch of <class 'dict'>. keys: input_ids, input_mask, segment_ids, unique_id<p>"]},{"cell_type":"code","metadata":{"id":"WjoqFyFxKTLz","colab_type":"code","outputId":"db01d5aa-c736-4dc0-a411-afd1e797452f","executionInfo":{"status":"ok","timestamp":1579003681750,"user_tz":480,"elapsed":164988,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["if True and verbose:\n","    # print out information from the first ds (batch)\n","    for d in ds:\n","        print(type(d), \"keys: \", d.keys())\n","        print(\"shape: \", d['unique_id'].shape, d['input_ids'].shape, d['input_mask'].shape, d['segment_ids'].shape)\n","        print(f\"\\nunique_id: {d['unique_id'][0]}\")\n","        print(f\"\\ninput_ids:\\n{d['input_ids'][0]}\")\n","        print(f\"\\ninput_mask:\\n{d['input_ids'][0]}\")\n","        print(f\"\\nsegment_ids:\\n{d['input_ids'][0]}\")\n","        break"],"execution_count":32,"outputs":[{"output_type":"stream","text":["<class 'dict'> keys:  dict_keys(['input_ids', 'input_mask', 'segment_ids', 'unique_id'])\n","shape:  (32,) (32, 512) (32, 512) (32, 512)\n","\n","unique_id: 5655493461695504401\n","\n","input_ids:\n","[  101   104  2029  2003  1996  2087  2691  2224  1997 23569  1011  1999\n","  1041  1011  5653  5821   102   259   107   260   159  1006  5342  1007\n","  2023  3720  2038  3674  3314  1012  3531  2393  5335  2009  2030  6848\n","  2122  3314  2006  1996  2831  3931  1012  1006  4553  2129  1998  2043\n","  2000  6366  2122 23561  7696  1007  2023  3720  3791  3176 22921  2005\n"," 22616  1012  3531  2393  5335  2023  3720  2011  5815 22921  2000 10539\n","  4216  1012  4895  6499  3126 11788  3430  2089  2022  8315  1998  3718\n","  1012  1006  2244  2297  1007  1006  4553  2129  1998  2043  2000  6366\n","  2023 23561  4471  1007  2023  3720  4298  3397  2434  2470  1012  3531\n","  5335  2009  2011 20410  2075  1996  4447  2081  1998  5815 23881 22921\n","  1012  8635  5398  2069  1997  2434  2470  2323  2022  3718  1012  1006\n","  2254  2325  1007  1006  4553  2129  1998  2043  2000  6366  2023 23561\n","  4471  1007  1006  4553  2129  1998  2043  2000  6366  2023 23561  4471\n","  1007   266   160  2112  1997  1037  2186  2006  4274  5821  3945  3194\n"," 20600  2334  3945  3194 23569 27605 26652  2591  2865  5821 10373  5821\n","  6523  7941  5821  4180  5821  3128  6475  3945  3194  5821  3477  1011\n","  2566  1011 11562  3465  2566  8605  3945 25095  4773 25095  4653  6475\n","  4748 10851  6123  8787  6475 14260 14126  8727  5821  3465  2566  2895\n","  6599  6631  4684  6475   289   109 10373  5821  2003  1996  2552  1997\n","  6016  1037  3293  4471  1010  4050  2000  1037  2177  1997  2111  1010\n","  2478 10373  1012  1999  2049  5041  4355  3168  1010  2296 10373  2741\n","  2000  1037  4022  2030  2783  8013  2071  2022  2641 10373  5821  1012\n","  2009  2788  7336  2478 10373  2000  4604 14389  1010  5227  2449  1010\n","  2030 14017 28775  2102  4341  2030 11440  1010  1998  2003  3214  2000\n","  3857  9721  1010  3404  1010  2030  4435  7073  1012  5821 22028  2064\n","  2022  2741  2000  1037  4156  2599  2862  2030  1037  2783  8013  7809\n","  1012  1996  2744  2788  5218  2000  6016 10373  7696  2007  1996  3800\n","  1997 20226  1037  6432  1005  1055  3276  2007  2783  2030  3025  6304\n","  1010 11434  8013  9721  1998  9377  2449  1010 13868  2047  6304  2030\n"," 13359  2783  6304  2000  5309  2242  3202  1010  1998  6631  2353  1011\n","  2283 14997  1012   290   110 10373  5821  2038  7964  5901  4077  1996\n"," 10660  3930  1997  1996  7398  2301  1012  3188  2000  2023  3930  1010\n","  2043 22028  2020  3117  7368  2000  1996  3484  1997  6304  1010 10373\n","  5821  2001  2025  2004  4621  1012  1999  3301  1010  5639 16215 13094\n","  2243  1997  3617  3941  3840  1006 11703  1007  2741  2041  1996  2034\n","  3742 10373  2000  3155  4278  4022  7846  3081  1996  3935  2470  3934\n","  4034  2897  1006 12098  9739  3388  1007  1012  2023 10373  4504  1999\n","  1002  2410  2454  4276  1997  4341  1999 11703  3688  1010  1998 11548\n","  1996  4022  1997  5821  2083  3742 22028  1012  2174  1010  2004 10373\n","  5821  2764  2004  2019  4621  2965  1997  3622  4807  1010  5198  2211\n"," 10851  2041  4180  2013 22028  2007 17736  1998 10851  3454  1012  1999\n","  2344  2000  6464 10639  1037  4471  2083 10373  1010  3006  2545  2018\n","  2000  4503  1037  2126  1997  6183  4180   102]\n","\n","input_mask:\n","[  101   104  2029  2003  1996  2087  2691  2224  1997 23569  1011  1999\n","  1041  1011  5653  5821   102   259   107   260   159  1006  5342  1007\n","  2023  3720  2038  3674  3314  1012  3531  2393  5335  2009  2030  6848\n","  2122  3314  2006  1996  2831  3931  1012  1006  4553  2129  1998  2043\n","  2000  6366  2122 23561  7696  1007  2023  3720  3791  3176 22921  2005\n"," 22616  1012  3531  2393  5335  2023  3720  2011  5815 22921  2000 10539\n","  4216  1012  4895  6499  3126 11788  3430  2089  2022  8315  1998  3718\n","  1012  1006  2244  2297  1007  1006  4553  2129  1998  2043  2000  6366\n","  2023 23561  4471  1007  2023  3720  4298  3397  2434  2470  1012  3531\n","  5335  2009  2011 20410  2075  1996  4447  2081  1998  5815 23881 22921\n","  1012  8635  5398  2069  1997  2434  2470  2323  2022  3718  1012  1006\n","  2254  2325  1007  1006  4553  2129  1998  2043  2000  6366  2023 23561\n","  4471  1007  1006  4553  2129  1998  2043  2000  6366  2023 23561  4471\n","  1007   266   160  2112  1997  1037  2186  2006  4274  5821  3945  3194\n"," 20600  2334  3945  3194 23569 27605 26652  2591  2865  5821 10373  5821\n","  6523  7941  5821  4180  5821  3128  6475  3945  3194  5821  3477  1011\n","  2566  1011 11562  3465  2566  8605  3945 25095  4773 25095  4653  6475\n","  4748 10851  6123  8787  6475 14260 14126  8727  5821  3465  2566  2895\n","  6599  6631  4684  6475   289   109 10373  5821  2003  1996  2552  1997\n","  6016  1037  3293  4471  1010  4050  2000  1037  2177  1997  2111  1010\n","  2478 10373  1012  1999  2049  5041  4355  3168  1010  2296 10373  2741\n","  2000  1037  4022  2030  2783  8013  2071  2022  2641 10373  5821  1012\n","  2009  2788  7336  2478 10373  2000  4604 14389  1010  5227  2449  1010\n","  2030 14017 28775  2102  4341  2030 11440  1010  1998  2003  3214  2000\n","  3857  9721  1010  3404  1010  2030  4435  7073  1012  5821 22028  2064\n","  2022  2741  2000  1037  4156  2599  2862  2030  1037  2783  8013  7809\n","  1012  1996  2744  2788  5218  2000  6016 10373  7696  2007  1996  3800\n","  1997 20226  1037  6432  1005  1055  3276  2007  2783  2030  3025  6304\n","  1010 11434  8013  9721  1998  9377  2449  1010 13868  2047  6304  2030\n"," 13359  2783  6304  2000  5309  2242  3202  1010  1998  6631  2353  1011\n","  2283 14997  1012   290   110 10373  5821  2038  7964  5901  4077  1996\n"," 10660  3930  1997  1996  7398  2301  1012  3188  2000  2023  3930  1010\n","  2043 22028  2020  3117  7368  2000  1996  3484  1997  6304  1010 10373\n","  5821  2001  2025  2004  4621  1012  1999  3301  1010  5639 16215 13094\n","  2243  1997  3617  3941  3840  1006 11703  1007  2741  2041  1996  2034\n","  3742 10373  2000  3155  4278  4022  7846  3081  1996  3935  2470  3934\n","  4034  2897  1006 12098  9739  3388  1007  1012  2023 10373  4504  1999\n","  1002  2410  2454  4276  1997  4341  1999 11703  3688  1010  1998 11548\n","  1996  4022  1997  5821  2083  3742 22028  1012  2174  1010  2004 10373\n","  5821  2764  2004  2019  4621  2965  1997  3622  4807  1010  5198  2211\n"," 10851  2041  4180  2013 22028  2007 17736  1998 10851  3454  1012  1999\n","  2344  2000  6464 10639  1037  4471  2083 10373  1010  3006  2545  2018\n","  2000  4503  1037  2126  1997  6183  4180   102]\n","\n","segment_ids:\n","[  101   104  2029  2003  1996  2087  2691  2224  1997 23569  1011  1999\n","  1041  1011  5653  5821   102   259   107   260   159  1006  5342  1007\n","  2023  3720  2038  3674  3314  1012  3531  2393  5335  2009  2030  6848\n","  2122  3314  2006  1996  2831  3931  1012  1006  4553  2129  1998  2043\n","  2000  6366  2122 23561  7696  1007  2023  3720  3791  3176 22921  2005\n"," 22616  1012  3531  2393  5335  2023  3720  2011  5815 22921  2000 10539\n","  4216  1012  4895  6499  3126 11788  3430  2089  2022  8315  1998  3718\n","  1012  1006  2244  2297  1007  1006  4553  2129  1998  2043  2000  6366\n","  2023 23561  4471  1007  2023  3720  4298  3397  2434  2470  1012  3531\n","  5335  2009  2011 20410  2075  1996  4447  2081  1998  5815 23881 22921\n","  1012  8635  5398  2069  1997  2434  2470  2323  2022  3718  1012  1006\n","  2254  2325  1007  1006  4553  2129  1998  2043  2000  6366  2023 23561\n","  4471  1007  1006  4553  2129  1998  2043  2000  6366  2023 23561  4471\n","  1007   266   160  2112  1997  1037  2186  2006  4274  5821  3945  3194\n"," 20600  2334  3945  3194 23569 27605 26652  2591  2865  5821 10373  5821\n","  6523  7941  5821  4180  5821  3128  6475  3945  3194  5821  3477  1011\n","  2566  1011 11562  3465  2566  8605  3945 25095  4773 25095  4653  6475\n","  4748 10851  6123  8787  6475 14260 14126  8727  5821  3465  2566  2895\n","  6599  6631  4684  6475   289   109 10373  5821  2003  1996  2552  1997\n","  6016  1037  3293  4471  1010  4050  2000  1037  2177  1997  2111  1010\n","  2478 10373  1012  1999  2049  5041  4355  3168  1010  2296 10373  2741\n","  2000  1037  4022  2030  2783  8013  2071  2022  2641 10373  5821  1012\n","  2009  2788  7336  2478 10373  2000  4604 14389  1010  5227  2449  1010\n","  2030 14017 28775  2102  4341  2030 11440  1010  1998  2003  3214  2000\n","  3857  9721  1010  3404  1010  2030  4435  7073  1012  5821 22028  2064\n","  2022  2741  2000  1037  4156  2599  2862  2030  1037  2783  8013  7809\n","  1012  1996  2744  2788  5218  2000  6016 10373  7696  2007  1996  3800\n","  1997 20226  1037  6432  1005  1055  3276  2007  2783  2030  3025  6304\n","  1010 11434  8013  9721  1998  9377  2449  1010 13868  2047  6304  2030\n"," 13359  2783  6304  2000  5309  2242  3202  1010  1998  6631  2353  1011\n","  2283 14997  1012   290   110 10373  5821  2038  7964  5901  4077  1996\n"," 10660  3930  1997  1996  7398  2301  1012  3188  2000  2023  3930  1010\n","  2043 22028  2020  3117  7368  2000  1996  3484  1997  6304  1010 10373\n","  5821  2001  2025  2004  4621  1012  1999  3301  1010  5639 16215 13094\n","  2243  1997  3617  3941  3840  1006 11703  1007  2741  2041  1996  2034\n","  3742 10373  2000  3155  4278  4022  7846  3081  1996  3935  2470  3934\n","  4034  2897  1006 12098  9739  3388  1007  1012  2023 10373  4504  1999\n","  1002  2410  2454  4276  1997  4341  1999 11703  3688  1010  1998 11548\n","  1996  4022  1997  5821  2083  3742 22028  1012  2174  1010  2004 10373\n","  5821  2764  2004  2019  4621  2965  1997  3622  4807  1010  5198  2211\n"," 10851  2041  4180  2013 22028  2007 17736  1998 10851  3454  1012  1999\n","  2344  2000  6464 10639  1037  4471  2083 10373  1010  3006  2545  2018\n","  2000  4503  1037  2126  1997  6183  4180   102]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QaM_Nysgxsob","colab_type":"text"},"source":["On full training set this will count to 284 and take about 600s (10 / 24s on the short data set)\n"]},{"cell_type":"code","metadata":{"id":"NT2UMXOZuY4p","colab_type":"code","outputId":"8cfb4888-faae-461a-b288-6c30ad17bb4b","executionInfo":{"status":"ok","timestamp":1579004286546,"user_tz":480,"elapsed":769768,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["result=model.predict(ds, verbose = 1 if verbose else 0)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["     76/Unknown - 604s 8s/step"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Vomo-MtL0S-X","colab_type":"text"},"source":["Save the results in a compressd file. Not sure if we are wanting to be able to analyze them<br>\n","later or reload them to save having to run top half of program when testing bottom half."]},{"cell_type":"code","metadata":{"id":"VkIgBtoIua2p","colab_type":"code","colab":{}},"source":["if False:\n","    print(\"Saving compressed results\")\n","    np.savez_compressed(f'{outdir}/bert-joint-baseline-output.npz',\n","                    **dict(zip(['unique_id','start_logits','end_logits','answer_type_logits'],\n","                               result)))   # result is a list of 4 np.ndarray"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w25G8rTa_rbo","colab_type":"code","outputId":"6ea0ceb8-a554-454f-e0d4-90d27f7f944c","executionInfo":{"status":"ok","timestamp":1579004286547,"user_tz":480,"elapsed":769748,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":[" if True and verbose:\n","    # print out the first result\n","    np.set_printoptions(suppress=True, precision=0)\n","    print(f\"result is a {type(result)} of len ({len(result)})\")\n","    print(f\"result[0] (unique_id) is a {type(result[0])} of shape {result[0].shape}\")\n","    print(f\"result[1] (start_logits) is a {type(result[1])} of shape {result[1].shape}\")\n","    print(f\"result[2] (end_logits) is a {type(result[2])} of shape {result[2].shape}\")\n","    print(f\"result[3] (answer_type_logits) is a {type(result[3])} of shape {result[3].shape}\")\n","    print(f\"\\nunique_id: \\n{result[0][0]}\")\n","    print(f\"\\nstart_logits: \\n{result[1][0]}\")\n","    print(f\"\\nend_logits: \\n{result[2][0]}\")\n","    print(f\"\\nanswer_type_logits: \\n{result[3][0]}\")\n"],"execution_count":35,"outputs":[{"output_type":"stream","text":["result is a <class 'list'> of len (4)\n","result[0] (unique_id) is a <class 'numpy.ndarray'> of shape (2405, 1)\n","result[1] (start_logits) is a <class 'numpy.ndarray'> of shape (2405, 512)\n","result[2] (end_logits) is a <class 'numpy.ndarray'> of shape (2405, 512)\n","result[3] (answer_type_logits) is a <class 'numpy.ndarray'> of shape (2405, 5)\n","\n","unique_id: \n","[5655493461695504401]\n","\n","start_logits: \n","[  2.  -9.  -8.  -9.  -9.  -9.  -9.  -9.  -9.  -7.  -9.  -9.  -8.  -8.\n","  -9.  -9.  -8.   2.  -1.  -1.  -1.  -2.  -1.  -2.  -2.  -2.  -2.  -2.\n","  -2.  -3.  -2.  -2.  -3.  -3.  -4.  -3.  -3.  -3.  -4.  -4.  -3.  -4.\n","  -4.  -3.  -3.  -4.  -4.  -3.  -4.  -3.  -4.  -3.  -4.  -4.  -4.  -3.\n","  -3.  -3.  -3.  -4.  -3.  -5.  -4.  -4.  -4.  -4.  -4.  -4.  -4.  -4.\n","  -4.  -4.  -4.  -5.  -3.  -6.  -6.  -5.  -4.  -4.  -5.  -4.  -5.  -4.\n","  -5.  -5.  -4.  -4.  -5.  -5.  -4.  -5.  -5.  -5.  -5.  -4.  -5.  -4.\n","  -5.  -5.  -5.  -4.  -5.  -4.  -4.  -4.  -5.  -4.  -4.  -5.  -5.  -4.\n","  -6.  -5.  -4.  -5.  -6.  -5.  -5.  -5.  -6.  -4.  -5.  -5.  -6.  -4.\n","  -4.  -5.  -6.  -5.  -6.  -5.  -5.  -6.  -6.  -6.  -5.  -6.  -6.  -6.\n","  -6.  -5.  -6.  -5.  -6.  -6.  -6.  -5.  -6.  -6.  -6.  -6.  -6.  -5.\n","  -5.  -6.  -5.  -3.  -5.  -3.  -5.  -4.  -4.  -4.  -3.  -4.  -3.  -4.\n","  -3.  -4.  -4.  -5.  -4.  -5.  -5.  -4.  -5.  -5.  -4.  -5.  -4.  -6.\n","  -5.  -4.  -6.  -4.  -4.  -4.  -5.  -5.  -3.  -7.  -6.  -7.  -6.  -4.\n","  -6.  -6.  -4.  -4.  -4.  -4.  -4.  -5.  -5.  -5.  -4.  -7.  -6.  -5.\n","  -6.  -5.  -6.  -5.  -7.  -6.  -5.  -6.  -5.  -5.  -6.  -5.   5.  -4.\n","  -3.   0.  -0.  -6.   1.  -1.  -0.  -2.  -5.  -2.  -2.  -1.  -2.  -7.\n","  -3.  -5.  -3.  -2.  -6.  -1.  -4.  -3.  -4.  -5.  -4.   1.  -1.  -2.\n","  -1.   0.  -1.  -7.  -2.  -3.  -4.  -6.  -4.  -0.  -4.  -5.  -1.   0.\n","  -2.   2.   1.   4.   3.   1.  -3.   1.  -3.  -6.  -4.   0.  -5.  -6.\n","  -1.  -7.  -3.  -6.  -5.  -1.   1.   3.   3.   2.  -7.  -2.  -8.  -6.\n","  -2.  -3.  -6.   0.  -1.   0.  -3.   0.  -0.   0.  -1.  -1.  -6.  -7.\n","  -2.  -2.  -2.  -4.  -6.  -1.  -2.  -1.  -3.  -4.   2.  -1.  -2.   1.\n","   1.   1.  -3.   3.  -2.  -1.  -8.  -6.  -1.  -5.  -3.  -8.  -5.  -4.\n","  -6.   2.  -4.  -3.  -8.  -4.  -5.  -7.   1.  -4.  -6.  -7.  -2.  -5.\n","  -5.  -7.  -5.  -6.  -6.  -8.  -8.  -2.  -3.  -8.  -6.  -4.  -4.  -8.\n","  -8.  -1.  -6.  -8.  -7.  -8.  -7.  -6.  -4.  -7.  -9.  -7.  -7.  -8.\n","  -9.  -6.  -9.  -9.  -7.  -9.  -7.  -4.  -8.  -5.  -8.  -8.  -7.  -7.\n","  -9.  -7.  -9.  -3.  -7.  -8.  -6.  -8.  -9.  -9.  -7.  -7.  -8.  -6.\n","  -8.  -8.  -9.  -9.  -5.  -8.  -8.  -9.  -8.  -9.  -7.  -8.  -6.  -6.\n","  -4.  -7.  -6.  -6.  -8.  -5.  -7.  -8.  -7.  -5.  -8.  -8.  -8.  -9.\n","  -9.  -8.  -9.  -9.  -9.  -9.  -8.  -5.  -7.  -7.  -5.  -9.  -9.  -9.\n","  -9.  -7.  -8.  -5.  -8.  -9.  -9.  -7.  -6.  -6.  -7.  -2.  -5.  -2.\n","  -5. -10.  -9.  -9.  -6.  -3.  -7.  -7.  -6.  -3.  -4.  -3.  -8.  -1.\n","  -3.  -9.  -3.  -7.  -3.  -8.  -5.  -8.  -5.  -8.  -5.  -8.  -6.  -7.\n","  -9.  -6.  -5.  -3.  -3.  -2.  -5.  -5.  -6.  -3.  -9.  -3.  -7.  -8.\n","  -7.  -7.  -6.  -7.  -9.  -4.  -5.  -8.]\n","\n","end_logits: \n","[ 2. -9. -9. -9. -9. -9. -9. -8. -9. -9. -8. -8. -9. -6. -7. -5. -6. -0.\n"," -1. -1. -1. -2. -1. -1. -1. -1. -2. -2. -1. -2. -2. -2. -2. -2. -3. -2.\n"," -2. -2. -3. -3. -3. -2. -3. -3. -2. -3. -4. -3. -3. -3. -3. -3. -3. -3.\n"," -3. -2. -3. -3. -2. -4. -2. -3. -3. -3. -3. -4. -3. -4. -3. -3. -4. -3.\n"," -3. -4. -3. -6. -6. -3. -3. -4. -4. -3. -4. -3. -4. -4. -4. -4. -4. -5.\n"," -4. -4. -5. -4. -4. -4. -4. -4. -4. -4. -4. -3. -4. -4. -3. -3. -5. -4.\n"," -3. -4. -4. -4. -4. -5. -4. -4. -5. -4. -4. -4. -5. -4. -4. -5. -5. -4.\n"," -4. -5. -5. -4. -5. -5. -5. -5. -5. -5. -4. -5. -5. -5. -5. -5. -5. -4.\n"," -5. -5. -5. -4. -5. -5. -5. -5. -5. -5. -4. -5. -3. -3. -4. -4. -4. -4.\n"," -3. -4. -3. -2. -3. -3. -3. -4. -4. -4. -5. -5. -3. -4. -4. -3. -4. -4.\n"," -5. -4. -4. -4. -4. -5. -3. -5. -4. -3. -4. -6. -6. -6. -4. -4. -6. -4.\n"," -4. -3. -4. -3. -4. -4. -4. -4. -5. -5. -4. -5. -4. -6. -4. -5. -6. -4.\n"," -5. -4. -5. -2. -5. -4. -4. -0. -6. -5. -5. -5. -4. -5. -2. -0. -5. -6.\n"," -4. -5. -4. -5.  1. -4. -5.  1. -2. -6. -6. -5. -6. -4. -5. -5. -3. -5.\n"," -4. -6. -5. -6. -4.  1. -6. -6. -6. -3.  0. -1. -6. -6. -6. -5. -1. -3.\n"," -3.  1. -2. -4. -3. -4. -4. -5. -6. -2. -2. -3.  4. -0. -5. -6. -5. -3.\n"," -3. -1. -5. -1. -5. -4. -3.  3. -0. -5. -3. -6. -6. -5. -5. -6. -5. -5.\n"," -3. -4. -5. -5. -4.  2. -1. -7. -6. -7. -6. -5. -4. -4. -1. -5. -5. -4.\n"," -4. -3. -5. -4. -6. -4. -4. -5. -6. -6. -5.  1. -3. -4. -4. -1. -5. -4.\n","  1. -3. -4. -5. -1. -3. -4. -4. -2. -5. -4. -4. -1. -4. -4. -3. -5. -6.\n"," -3.  3.  4. -6. -6. -5. -3. -6. -6. -6. -7. -8. -6. -5. -7. -8. -7. -5.\n"," -6. -8. -8. -8. -6. -7. -9. -5. -8. -7. -5. -8. -9. -8. -8. -4. -8. -5.\n"," -3. -8. -7. -8. -5. -7. -8. -7. -8. -8. -9. -8. -7. -8. -8. -7. -7. -8.\n"," -8. -7. -8. -8. -8. -8. -7. -6. -8. -9. -8. -7. -3. -8. -8. -8. -7. -7.\n"," -8. -7. -8. -9. -8. -7. -5. -7. -8. -6. -8. -8. -8. -8. -7. -7. -8. -5.\n"," -8. -8. -3. -7. -7. -8. -9. -8. -8. -3. -8. -6. -2. -6. -8. -8. -9. -6.\n"," -4. -7. -8. -7. -6. -6. -7. -4. -1. -6. -8. -8. -6. -7. -4. -7. -4. -7.\n"," -6. -7. -5. -2. -5. -9. -8. -8. -7. -5. -8. -3. -7. -2. -6. -8. -6. -8.\n"," -8. -8. -9. -8. -7. -7. -4. -6.]\n","\n","answer_type_logits: \n","[ 0. -4. -4.  3.  3.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S9WQFNVPuctX","colab_type":"code","colab":{}},"source":["Span = collections.namedtuple(\"Span\", [\"start_token_idx\", \"end_token_idx\", \"score\"])   ## Added score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5vL3t4j5ufId","colab_type":"code","colab":{}},"source":["class ScoreSummary(object):\n","  def __init__(self):\n","    self.predicted_label = None\n","    self.short_span_score = None\n","    self.cls_token_score = None\n","    self.answer_type_logits = None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5kR_ixQEuhmc","colab_type":"code","colab":{}},"source":["class EvalExample(object):\n","  \"\"\"Eval data available for a single example.\"\"\"\n","  def __init__(self, example_id, candidates):\n","    self.example_id = example_id\n","    self.candidates = candidates\n","    self.results = {}\n","    self.features = {}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nLoKGECulk3","colab_type":"code","colab":{}},"source":["def get_best_indexes(logits, n_best_size):\n","  \"\"\"Get the n-best logits from a list.\"\"\"\n","  index_and_score = sorted(\n","      enumerate(logits[1:], 1), key=lambda x: x[1], reverse=True)\n","  best_indexes = []\n","  for i in range(len(index_and_score)):\n","    if i >= n_best_size:\n","      break\n","    best_indexes.append(index_and_score[i][0])\n","  return best_indexes\n","\n","def top_k_indices(logits,n_best_size,token_map):\n","    indices = np.argsort(logits[1:])+1\n","    indices = indices[token_map[indices]!=-1]\n","    return indices[-n_best_size:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANbolDyakWHz","colab_type":"code","colab":{}},"source":["def remove_duplicates(span):\n","    start_end = []\n","    for s in span:\n","        cont = 0\n","        if not start_end:\n","            start_end.append(Span(s[0], s[1], s[2]))\n","            cont += 1\n","        else:\n","            for i in range(len(start_end)):\n","                if start_end[i][0] == s[0] and start_end[i][1] == s[1]:\n","                    cont += 1\n","        if cont == 0:\n","            start_end.append(Span(s[0], s[1], s[2]))\n","            \n","    return start_end"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MxkKIIA4kbBK","colab_type":"code","colab":{}},"source":["def get_short_long_span(predictions, example):\n","    \n","    sorted_predictions = sorted(predictions, reverse=True)\n","    short_span = []\n","    long_span = []\n","    for prediction in sorted_predictions:\n","        score, _, summary, start_span, end_span = prediction\n","        # get scores > zero\n","        if score > 0:\n","            short_span.append(Span(int(start_span), int(end_span), float(score)))\n","\n","    short_span = remove_duplicates(short_span)\n","\n","    for s in range(len(short_span)):\n","        for c in example.candidates:\n","            start = short_span[s].start_token_idx\n","            end = short_span[s].end_token_idx\n","            ## print(c['top_level'],c['start_token'],start,c['end_token'],end)\n","            if c[\"top_level\"] and c[\"start_token\"] <= start and c[\"end_token\"] >= end:\n","                long_span.append(Span(int(c[\"start_token\"]), int(c[\"end_token\"]), float(short_span[s].score)))\n","                break\n","    long_span = remove_duplicates(long_span)\n","    \n","    if not long_span:\n","        long_span = [Span(-1, -1, -10000.0)]\n","    if not short_span:\n","        short_span = [Span(-1, -1, -10000.0)]\n","        \n","    \n","    return short_span, long_span"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e11n9VMvupHr","colab_type":"text"},"source":["### Understanding the code\n","In the item \"answer_type\", in the last lines of this block, it is responsible for storing the identified response type, which, according to [github project repository](https://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py) can be:\n","1. UNKNOWN = 0\n","2. YES = 1\n","3. NO = 2\n","4. SHORT = 3\n","5. LONG = 4\n","\n","#### Note from ChrisM\n","The other thing that compute_predictions does is look through the logits for the start and end indexes and finds the pairs of start/end indexes which seem most likely to be the short answer.  It does this by creating a score for the short answers based on the combined start + end logit scores and for some reason subtracting from this the logit scores for the initial CLS token in the Bert input.  The long span is determined from the best short answer.\n","\n","I am wondering here if we might be able to get some improvement by building an additional model to select the best short span in some other way rather than just taking the max logit values.  Maybe a tree ensemble, for example, could look at the logits in a different way and make a better decision for the short and long answers than what is coded here."]},{"cell_type":"code","metadata":{"id":"4GNEaA1QusjQ","colab_type":"code","colab":{}},"source":["def compute_predictions(example):\n","    \"\"\"Converts an example into an NQEval object for evaluation.\"\"\"\n","    predictions = []\n","    n_best_size = FLAGS.n_best_size\n","    max_answer_length = FLAGS.max_answer_length\n","    i = 0\n","    for unique_id, result in example.results.items():\n","        if unique_id not in example.features:\n","            raise ValueError(\"No feature found with unique_id:\", unique_id)\n","        token_map = np.array(example.features[unique_id][\"token_map\"]) #.int64_list.value\n","        start_indexes = top_k_indices(result.start_logits,n_best_size,token_map)\n","        if len(start_indexes)==0:\n","            continue\n","        end_indexes   = top_k_indices(result.end_logits,n_best_size,token_map)\n","        if len(end_indexes)==0:\n","            continue\n","        indexes = np.array(list(np.broadcast(start_indexes[None],end_indexes[:,None])))  \n","        indexes = indexes[(indexes[:,0]<indexes[:,1])*(indexes[:,1]-indexes[:,0]<max_answer_length)]\n","        for _, (start_index,end_index) in enumerate(indexes):  \n","            summary = ScoreSummary()\n","            summary.short_span_score = (\n","                result.start_logits[start_index] +\n","                result.end_logits[end_index])\n","            summary.cls_token_score = (\n","                result.start_logits[0] + result.end_logits[0])\n","            summary.answer_type_logits = result.answer_type_logits-result.answer_type_logits.mean()\n","            start_span = token_map[start_index]\n","            end_span = token_map[end_index] + 1\n","\n","            # Span logits minus the cls logits seems to be close to the best.\n","            score = summary.short_span_score - summary.cls_token_score\n","            predictions.append((score, i, summary, start_span, end_span))\n","            i += 1 # to break ties\n","\n","    # Default empty prediction.\n","    #score = -10000.0\n","    short_span = [Span(-1, -1, -10000.0)]\n","    long_span  = [Span(-1, -1, -10000.0)]\n","    summary    = ScoreSummary()\n","\n","    if predictions:\n","        short_span, long_span = get_short_long_span(predictions, example)\n","      \n","    summary.predicted_label = {\n","        \"example_id\": int(example.example_id),\n","        \"long_answers\": {\n","          \"tokens_and_score\": long_span,\n","          #\"end_token\": long_span,\n","          \"start_byte\": -1,\n","          \"end_byte\": -1\n","        },\n","        #\"long_answer_score\": answer_score,\n","        \"short_answers\": {\n","          \"tokens_and_score\": short_span,\n","          #\"end_token\": short_span,\n","          \"start_byte\": -1,\n","          \"end_byte\": -1,\n","          \"yes_no_answer\": \"NONE\"\n","        }\n","        #\"short_answer_score\": answer_scores,\n","        \n","        #\"answer_type_logits\": summary.answer_type_logits.tolist(),\n","        #\"answer_type\": int(np.argmax(summary.answer_type_logits))\n","       }\n","\n","    return summary"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ww0cVDwMuv62","colab_type":"code","colab":{}},"source":["def compute_pred_dict(candidates_dict, dev_features, raw_results,tqdm=None):\n","    \"\"\"Computes official answer key from raw logits.\"\"\"\n","    raw_results_by_id = [(int(res.unique_id),1, res) for res in raw_results]\n","\n","    examples_by_id = [(int(k),0,v) for k, v in candidates_dict.items()]\n","  \n","    features_by_id = [(int(d['unique_id']),2,d) for d in dev_features] \n","  \n","    # ChrisM Note    (Join examples with features and raw results.)\n","    # NOTE:  this strange looking merge where we are sorting tuples is intended.  \n","    #        In the examples_by_id, each question / document pair has an id.\n","    #        Then, in the raw_results_by_id, a document with id 'i' from the examples_by_id\n","    #        will be split across multiple raw results with ids: i, i+1, i+2, ...\n","    #        The features_by_id has the same structure as raw_results_by_id and \n","    #        contains a map from Bert-Joint sequence tokens to whitespace delimited\n","    #        words in the original document ( which is what we report in the answer )\n","    examples = []\n","    print('merging examples...')\n","    merged = sorted(examples_by_id + raw_results_by_id + features_by_id)\n","    print('done.')\n","    for idx, type_, datum in merged:\n","        if type_==0: #isinstance(datum, list):\n","            examples.append(EvalExample(idx, datum))\n","        elif type_==2: #\"token_map\" in datum:\n","            examples[-1].features[idx] = datum\n","        else:\n","            examples[-1].results[idx] = datum\n","\n","    # Construct prediction objects.\n","    print('Computing predictions...')\n","   \n","    nq_pred_dict = {}\n","    #summary_dict = {}\n","    if tqdm is not None:\n","        examples = tqdm(examples)\n","    for e in examples:\n","        summary = compute_predictions(e)\n","        #summary_dict[e.example_id] = summary\n","        nq_pred_dict[e.example_id] = summary.predicted_label\n","    return nq_pred_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wE-BkBWhuzMt","colab_type":"code","colab":{}},"source":["def read_candidates_from_one_split(input_path):\n","  \"\"\"Read candidates from a single jsonl file.\"\"\"\n","  candidates_dict = {}\n","  print(\"Reading examples from: %s\" % input_path)\n","  if input_path.endswith(\".gz\"):\n","    with gzip.GzipFile(fileobj=tf.io.gfile.GFile(input_path, \"rb\")) as input_file:\n","      for index, line in enumerate(input_file):\n","        e = json.loads(line)\n","        candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n","        \n","  else:\n","    with tf.io.gfile.GFile(input_path, \"r\") as input_file:\n","      for index, line in enumerate(input_file):\n","        e = json.loads(line)\n","        candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]  # testar juntando com question_text\n","  return candidates_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DmBJNth5u04G","colab_type":"code","colab":{}},"source":["def read_candidates(input_pattern):\n","  \"\"\"Read candidates with real multiple processes.\"\"\"\n","  input_paths = tf.io.gfile.glob(input_pattern)\n","  final_dict = {}\n","  for input_path in input_paths:\n","    final_dict.update(read_candidates_from_one_split(input_path))\n","  return final_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pYqeZlaLu2uR","colab_type":"code","outputId":"61f261bb-52ae-444d-9db4-c03cab8a0e9f","executionInfo":{"status":"ok","timestamp":1579004289746,"user_tz":480,"elapsed":772833,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":215,"referenced_widgets":["906bfcecf750402c81e00a5a3e849191","23390be1425546789971d760dc12b605","910688a37f75413c96ff0b322e8352f8","09f616deb510420885275d15298c8a9f","9020a00282674cfab4ddd2af4f2ed110","bf1e882f4a004b6faccb9569fc18851f","63aed3dcbf04486db71bfd2729d8852c","ce6ccf9738dd4097b902154b19fe0258"]}},"source":["all_results = [bert_utils.RawResult(*x) for x in zip(*result)]\n","    \n","print (\"About to read_candidates()\")\n","\n","candidates_dict = read_candidates(f\"{test_file}\")\n","\n","print (\"setting up eval_features as list\")\n","\n","eval_features = list(token_map_ds)\n","\n","print (\"going to compute_pred_dict()\")\n","\n","tqdm_notebook= tqdm.tqdm_notebook\n","nq_pred_dict = compute_pred_dict(candidates_dict, \n","                                       eval_features,\n","                                       all_results,\n","                                      tqdm=tqdm_notebook)\n","\n","predictions_json = {\"predictions\": list(nq_pred_dict.values())}\n","\n","print (\"about to write predictions.json\")\n","\n","with tf.io.gfile.GFile(f\"{outdir}/predictions.json\", \"w\") as f:\n","    json.dump(predictions_json, f, indent=4)\n","print('done writing!')"],"execution_count":46,"outputs":[{"output_type":"stream","text":["About to read_candidates()\n","Reading examples from: /content/data/tensorflow2-question-answering/simplified-nq-eval.jsonl\n","setting up eval_features as list\n","going to compute_pred_dict()\n","merging examples...\n","done.\n","Computing predictions...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"906bfcecf750402c81e00a5a3e849191","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","about to write predictions.json\n","done writing!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ore6D3Fku6oY","colab_type":"text"},"source":["### Processing the Output\n"]},{"cell_type":"markdown","metadata":{"id":"sTdPRVxZvIRK","colab_type":"text"},"source":["#### Filtering the Answers"]},{"cell_type":"code","metadata":{"id":"xQsvlqrGC_mf","colab_type":"code","outputId":"07b9e916-8350-4410-e036-d7cfd97c5493","executionInfo":{"status":"ok","timestamp":1579004289747,"user_tz":480,"elapsed":772824,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["answers_df = pd.read_json(f\"{outdir}/predictions.json\")\n","answers_df.head()"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'example_id': -8799945603687418006, 'long_ans...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'example_id': -8627347779381584683, 'long_ans...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>{'example_id': -8114175076810279695, 'long_ans...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>{'example_id': -8062182676792486818, 'long_ans...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>{'example_id': -7766157450214546755, 'long_ans...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         predictions\n","0  {'example_id': -8799945603687418006, 'long_ans...\n","1  {'example_id': -8627347779381584683, 'long_ans...\n","2  {'example_id': -8114175076810279695, 'long_ans...\n","3  {'example_id': -8062182676792486818, 'long_ans...\n","4  {'example_id': -7766157450214546755, 'long_ans..."]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"x2yo3_ToC_mg","colab_type":"code","colab":{}},"source":["# {long score > 2, cont = 5 | short score > 2, cont = 5} = 0.18\n","# { long score > 2, cont = 5 | short score > 6, cont = 5}\n","# { long score > 2, cont = 1 | short score > 6, cont = 5}\n","\n","def df_long_index_score(df):\n","    answers = []\n","    cont = 0\n","    for e in df['long_answers']['tokens_and_score']:\n","        # if score > 2\n","        if e[2] > 3: \n","            index = {}\n","            index['start'] = e[0]\n","            index['end'] = e[1]\n","            index['score'] = e[2]\n","            answers.append(index)\n","            cont += 1\n","        # number of answers\n","        if cont == 1:\n","            break\n","            \n","    return answers\n","\n","def df_short_index_score(df):\n","    answers = []\n","    cont = 0\n","    for e in df['short_answers']['tokens_and_score']:\n","        # if score > 2\n","        if e[2] > 8:\n","            index = {}\n","            index['start'] = e[0]\n","            index['end'] = e[1]\n","            index['score'] = e[2]\n","            answers.append(index)\n","            cont += 1\n","        # number of answers\n","        if cont == 1:\n","            break\n","            \n","    return answers\n","\n","def df_example_id(df):\n","    return df['example_id']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o0fvbLOsC_mi","colab_type":"code","outputId":"f92c1a04-8f65-4703-be8f-6960a7f0df31","executionInfo":{"status":"ok","timestamp":1579004289748,"user_tz":480,"elapsed":772798,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["answers_df['example_id'] = answers_df['predictions'].apply(df_example_id)\n","\n","answers_df['long_indexes_and_scores'] = answers_df['predictions'].apply(df_long_index_score)\n","\n","answers_df['short_indexes_and_scores'] = answers_df['predictions'].apply(df_short_index_score)\n","\n","answers_df.head()"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>predictions</th>\n","      <th>example_id</th>\n","      <th>long_indexes_and_scores</th>\n","      <th>short_indexes_and_scores</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'example_id': -8799945603687418006, 'long_ans...</td>\n","      <td>-8799945603687418006</td>\n","      <td>[{'start': 122, 'end': 348, 'score': 6.9993805...</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'example_id': -8627347779381584683, 'long_ans...</td>\n","      <td>-8627347779381584683</td>\n","      <td>[{'start': 243, 'end': 474, 'score': 7.1751813...</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>{'example_id': -8114175076810279695, 'long_ans...</td>\n","      <td>-8114175076810279695</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>{'example_id': -8062182676792486818, 'long_ans...</td>\n","      <td>-8062182676792486818</td>\n","      <td>[{'start': 553, 'end': 629, 'score': 12.610366...</td>\n","      <td>[{'start': 565, 'end': 567, 'score': 12.610366...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>{'example_id': -7766157450214546755, 'long_ans...</td>\n","      <td>-7766157450214546755</td>\n","      <td>[{'start': 178, 'end': 233, 'score': 4.7214059...</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         predictions  ...                           short_indexes_and_scores\n","0  {'example_id': -8799945603687418006, 'long_ans...  ...                                                 []\n","1  {'example_id': -8627347779381584683, 'long_ans...  ...                                                 []\n","2  {'example_id': -8114175076810279695, 'long_ans...  ...                                                 []\n","3  {'example_id': -8062182676792486818, 'long_ans...  ...  [{'start': 565, 'end': 567, 'score': 12.610366...\n","4  {'example_id': -7766157450214546755, 'long_ans...  ...                                                 []\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"bbH7afGFC_mk","colab_type":"code","outputId":"c53196bb-b1ab-4a25-b328-89d6d5bb7821","executionInfo":{"status":"ok","timestamp":1579004289749,"user_tz":480,"elapsed":772788,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["answers_df = answers_df.drop(['predictions'], axis=1)\n","answers_df.head()"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>example_id</th>\n","      <th>long_indexes_and_scores</th>\n","      <th>short_indexes_and_scores</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-8799945603687418006</td>\n","      <td>[{'start': 122, 'end': 348, 'score': 6.9993805...</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-8627347779381584683</td>\n","      <td>[{'start': 243, 'end': 474, 'score': 7.1751813...</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-8114175076810279695</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-8062182676792486818</td>\n","      <td>[{'start': 553, 'end': 629, 'score': 12.610366...</td>\n","      <td>[{'start': 565, 'end': 567, 'score': 12.610366...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-7766157450214546755</td>\n","      <td>[{'start': 178, 'end': 233, 'score': 4.7214059...</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            example_id  ...                           short_indexes_and_scores\n","0 -8799945603687418006  ...                                                 []\n","1 -8627347779381584683  ...                                                 []\n","2 -8114175076810279695  ...                                                 []\n","3 -8062182676792486818  ...  [{'start': 565, 'end': 567, 'score': 12.610366...\n","4 -7766157450214546755  ...                                                 []\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"8EswWi9WC_mm","colab_type":"code","colab":{}},"source":["def create_answer(entry):\n","    answer = []\n","    for e in entry:\n","        answer.append(str(e['start']) + ':'+ str(e['end']))\n","    if not answer:\n","        answer = \"\"\n","    return \", \".join(answer)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":false,"id":"-gw-QD-NC_mo","colab_type":"code","outputId":"a2ec1c3e-afdd-4f37-8048-65f0f43f0768","executionInfo":{"status":"ok","timestamp":1579004289750,"user_tz":480,"elapsed":772769,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["answers_df[\"long_answer\"] = answers_df['long_indexes_and_scores'].apply(create_answer)\n","answers_df[\"short_answer\"] = answers_df['short_indexes_and_scores'].apply(create_answer)\n","answers_df[\"example_id\"] = answers_df['example_id'].apply(lambda q: str(q))\n","\n","long_answers = dict(zip(answers_df[\"example_id\"], answers_df[\"long_answer\"]))\n","short_answers = dict(zip(answers_df[\"example_id\"], answers_df[\"short_answer\"]))\n","\n","answers_df.head()"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>example_id</th>\n","      <th>long_indexes_and_scores</th>\n","      <th>short_indexes_and_scores</th>\n","      <th>long_answer</th>\n","      <th>short_answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-8799945603687418006</td>\n","      <td>[{'start': 122, 'end': 348, 'score': 6.9993805...</td>\n","      <td>[]</td>\n","      <td>122:348</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-8627347779381584683</td>\n","      <td>[{'start': 243, 'end': 474, 'score': 7.1751813...</td>\n","      <td>[]</td>\n","      <td>243:474</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-8114175076810279695</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-8062182676792486818</td>\n","      <td>[{'start': 553, 'end': 629, 'score': 12.610366...</td>\n","      <td>[{'start': 565, 'end': 567, 'score': 12.610366...</td>\n","      <td>553:629</td>\n","      <td>565:567</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-7766157450214546755</td>\n","      <td>[{'start': 178, 'end': 233, 'score': 4.7214059...</td>\n","      <td>[]</td>\n","      <td>178:233</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             example_id  ... short_answer\n","0  -8799945603687418006  ...             \n","1  -8627347779381584683  ...             \n","2  -8114175076810279695  ...             \n","3  -8062182676792486818  ...      565:567\n","4  -7766157450214546755  ...             \n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"KpVctrr1C_mq","colab_type":"code","outputId":"83f7af4f-24a4-49b6-f89f-c1a0ab3ca354","executionInfo":{"status":"ok","timestamp":1579004289751,"user_tz":480,"elapsed":772760,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["answers_df = answers_df.drop(['long_indexes_and_scores', 'short_indexes_and_scores'], axis=1)\n","answers_df.head()"],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>example_id</th>\n","      <th>long_answer</th>\n","      <th>short_answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-8799945603687418006</td>\n","      <td>122:348</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-8627347779381584683</td>\n","      <td>243:474</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-8114175076810279695</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-8062182676792486818</td>\n","      <td>553:629</td>\n","      <td>565:567</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-7766157450214546755</td>\n","      <td>178:233</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             example_id long_answer short_answer\n","0  -8799945603687418006     122:348             \n","1  -8627347779381584683     243:474             \n","2  -8114175076810279695                         \n","3  -8062182676792486818     553:629      565:567\n","4  -7766157450214546755     178:233             "]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"KcQGwTpiC_ms","colab_type":"text"},"source":["### Generating the Submission File\n","\n","sample_submission.csv has to have two lines for every line in {test_file} and example_id must match."]},{"cell_type":"code","metadata":{"_kg_hide-input":false,"id":"-9YI_pNjC_ms","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":83},"outputId":"82f46d39-bdc2-4254-fa60-daa35ab1eef8","executionInfo":{"status":"ok","timestamp":1579004289752,"user_tz":480,"elapsed":772752,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}}},"source":["sample_submission = pd.read_csv(f\"{datadir}/{competition}/sample_submission.csv\")\n","\n","print(f\"Size of sample_submission: {len(sample_submission)} (sum of long and short answers)\")\n","print(f\"Number of long_answers: {len(long_answers)}\")\n","print(f\"Number of short_answers: {len(short_answers)}\")\n","print()\n","\n","long_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_long\")].apply(lambda q: long_answers[q[\"example_id\"].replace(\"_long\", \"\")], axis=1)\n","short_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_short\")].apply(lambda q: short_answers[q[\"example_id\"].replace(\"_short\", \"\")], axis=1)\n","\n","sample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_long\"), \"PredictionString\"] = long_prediction_strings\n","sample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_short\"), \"PredictionString\"] = short_prediction_strings\n"],"execution_count":54,"outputs":[{"output_type":"stream","text":["Size of sample_submission: 200 (sum of long and short answers)\n","Number of long_answers: 100\n","Number of short_answers: 100\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1QR5hG_rC_mu","colab_type":"code","colab":{}},"source":["## Deliver compiled submission results\n","if kernel == 'Colab':\n","    sample_submission.to_csv(f'{outdir}/submission.csv', index=False)\n","else:\n","    # Kaggle wants submission.csv dropped in cwd\n","    sample_submission.to_csv('submission.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1T9AiLSyCblp","colab_type":"code","outputId":"aeeb9fa7-f0ed-456e-d34e-f23b78147478","executionInfo":{"status":"ok","timestamp":1579004292846,"user_tz":480,"elapsed":775823,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["! zdump PST"],"execution_count":56,"outputs":[{"output_type":"stream","text":["PST  Tue Jan 14 12:18:11 2020 PST\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kO61eJOOC_mv","colab_type":"code","outputId":"74d9404e-0835-41e9-b5c0-da2dcec12384","executionInfo":{"status":"ok","timestamp":1579004292848,"user_tz":480,"elapsed":775815,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}},"colab":{"base_uri":"https://localhost:8080/","height":406}},"source":["sample_submission"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>example_id</th>\n","      <th>PredictionString</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5655493461695504401_long</td>\n","      <td>1952:2019</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5655493461695504401_short</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5328212470870865242_long</td>\n","      <td>212:310</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5328212470870865242_short</td>\n","      <td>213:215</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4435104480114867852_long</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>-6753967926867752330_short</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>-6874546130423309582_long</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>-6874546130423309582_short</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>67874408688779239_long</td>\n","      <td>190:309</td>\n","    </tr>\n","    <tr>\n","      <th>199</th>\n","      <td>67874408688779239_short</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200 rows × 2 columns</p>\n","</div>"],"text/plain":["                     example_id PredictionString\n","0      5655493461695504401_long        1952:2019\n","1     5655493461695504401_short                 \n","2      5328212470870865242_long          212:310\n","3     5328212470870865242_short          213:215\n","4      4435104480114867852_long                 \n","..                          ...              ...\n","195  -6753967926867752330_short                 \n","196   -6874546130423309582_long                 \n","197  -6874546130423309582_short                 \n","198      67874408688779239_long          190:309\n","199     67874408688779239_short                 \n","\n","[200 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7OK-KqArvjKu"},"source":["#&gt;&gt; Deleted to end before submitting to competition &lt;&lt;\n","<Details> This is the end of the project notebook. The rest of this notebook is helpful for<br>\n","development but should not be submitted to competition</Details>"]},{"cell_type":"markdown","metadata":{"id":"2K8xraR8bkWK","colab_type":"text"},"source":["### Check Scoring w/ nq_eval by Reading predictions.json\n","This is obviously not working. The way scores are stored in predictions.json changed and is no longe compatible<br>\n","with natural_questions.eval_utils. Look in that file for notes around line 222.\n","Amoung other potential problems, the<br>\n","conversion from predictions.json to nq_pred_dict does not do anything to evaluate different long_answer<br>\n","candidates and short_answer candidates to see what would be the best answer. Also, the way scores were tracked<br>\n","was changed and is likely not correct."]},{"cell_type":"code","metadata":{"id":"YQorIRRJbi6N","colab_type":"code","colab":{}},"source":["if False:\n","    import natural_questions.eval_utils as utils\n","    import natural_questions.nq_eval as nq_eval\n","    import flag_defaults\n","\n","    if kernel == \"Colab\":               # Kaggle is V2 by default\n","        #magic to make Colab path to Tensorflow V2 on Colab\n","        %tensorflow_version 2.x \n","\n","    import tensorflow as tf\n","    print(\"TensorfFlow\", tf.__version__)\n","    flags = tf.compat.v1.flags\n","    FLAGS = flags.FLAGS\n","\n","    flag_defaults.setflags()\n","\n","    flags.DEFINE_integer(\n","        'long_non_null_threshold', 2,\n","        'Require this many non-null long answer annotations to count gold as containing a long answer.')\n","    flags.DEFINE_integer(\n","        'short_non_null_threshold', 2,\n","        'Require this many non-null short answer annotations to count gold as containing a short answer.')\n","    flags.DEFINE_string(\n","        'gold_path', None, 'Path to the gzip JSON data. For '\n","        'multiple files, should be a glob pattern (e.g. \"/path/to/files-*\"')\n","    flags.DEFINE_string('predictions_path', None, 'Path to prediction JSON.')\n","    flags.DEFINE_bool(\n","        'cache_gold_data', False,\n","        'Whether to cache gold data in Pickle format to speed up multiple evaluations.')\n","    flags.DEFINE_integer('num_threads', 10, 'Number of threads for reading.')\n","    flags.DEFINE_bool('pretty_print', False, 'Whether to pretty print output.')\n","\n","    nq_gold_dict = utils.read_annotation(f\"{test_file}\", n_threads=10)\n","\n","    nq_pred_dict = utils.read_prediction_json( f\"{outdir}/predictions.json\")\n","\n","    long_answer_stats, short_answer_stats = nq_eval.score_answers(nq_gold_dict, nq_pred_dict)\n","\n","    metrics = nq_eval.get_metrics_with_answer_stats(long_answer_stats,\n","                                                short_answer_stats)\n","    print(*metrics.items(), sep='\\n')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xudwdoUxxpSn","colab_type":"text"},"source":["<font color=\"pink\"> Can we write nq_gold_dict and nq_pred_dict out to a file to evaluate where they are different?</font><br>\n","Might not mean a lot because my code for reading predictions.json is questionable."]},{"cell_type":"markdown","metadata":{"id":"97jslclD7upv","colab_type":"text"},"source":["### Check scoring with eaval script\n","from: https://www.kaggle.com/kenkrige/possible-evaluation-metric"]},{"cell_type":"code","metadata":{"id":"hfK1NM-m7t-M","colab_type":"code","colab":{}},"source":["def long_annotations(example):\n","    longs = [('%s:%s' % (l['start_token'],l['end_token']))\n","                for l in [a['long_answer'] for a in example['annotations']]\n","                if not l['candidate_index'] == -1\n","            ]\n","    return longs #list of long annotations\n","\n","def short_annotations(example):\n","    shorts = [('%s:%s' % (s['start_token'],s['end_token']))\n","              for s in \n","              # sum(list_of_lists, []) is not very efficient gives an easy flat map for short lists\n","              sum([a['short_answers'] for a in example['annotations']], [])\n","             ]\n","    return shorts #list of short annotations\n","\n","def yes_nos(example):\n","    return [\n","        yesno for yesno in [a['yes_no_answer'] for a in example['annotations']]\n","        if not yesno == 'NONE'\n","    ]\n","\n","    # This is the critical method where I guess at the competition metric.\n","class Score():\n","    def __init__(self):\n","        self.TP = 0\n","        self.FP = 0\n","        self.FN = 0\n","        self.TN = 0\n","    def F1(self):\n","        return 2 * self.TP / (2 * self.TP + self.FP + self.FN)\n","    def increment(self, prediction, annotations, yes_nos):\n","        if prediction in yes_nos:\n","            print(prediction, yes_nos)\n","            self.TP += 1\n","        elif len(prediction) > 0:\n","            if prediction in annotations:\n","                self.TP += 1\n","            else:\n","                self.FP += 1\n","        elif len(annotations) == 0:\n","            self.TN += 1\n","        else:\n","            self.FN +=1\n","    def scores(self):\n","        return 'TP = {}   FP = {}   FN = {}   TN = {}   F1 = {:.2f}'.format(\n","            self.TP, self.FP, self.FN, self.TN, self.F1())\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OzcclJ4L8H8h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66},"outputId":"5a0a04f0-cc4a-4a07-852f-a1f37740a51b","executionInfo":{"status":"ok","timestamp":1579004293152,"user_tz":480,"elapsed":776089,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}}},"source":["## He called this predictions but I think he means submission\n","#  predictions = pd.read_csv('../input/tinydev/ken_predictions.csv', na_filter=False).set_index('example_id')\n","#  This file should have been created from your reserved records in nq-dev-sample.jsonl (your training file)\n","submission = pd.read_csv(f\"{outdir}/submission.csv\", na_filter=False).set_index('example_id')\n","\n","long_score = Score()\n","short_score = Score()\n","total_score = Score()\n","for example in map(json.loads, open(f\"{test_file}\", 'r')):\n","    long_pred = submission.loc[str(example['example_id']) + '_long', 'PredictionString']\n","    long_score.increment(long_pred, long_annotations(example), [])\n","    total_score.increment(long_pred, long_annotations(example), [])\n","    short_pred = submission.loc[str(example['example_id']) + '_short', 'PredictionString']\n","    short_score.increment(short_pred, short_annotations(example), yes_nos(example))\n","    total_score.increment(short_pred, short_annotations(example), [])\n","\n","print(\"short_scores:\", short_score.scores())\n","print(\"long_scores:\", long_score.scores())\n","print(\"total_scores:\", total_score.scores(), '(LB score)')\n"],"execution_count":60,"outputs":[{"output_type":"stream","text":["short_scores: TP = 15   FP = 23   FN = 9   TN = 53   F1 = 0.48\n","long_scores: TP = 35   FP = 35   FN = 6   TN = 24   F1 = 0.63\n","total_scores: TP = 50   FP = 58   FN = 15   TN = 77   F1 = 0.58 (LB score)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6Udo19R6y9LD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":161},"outputId":"86fe0bd3-01bc-4be9-894a-3da2b91be375","executionInfo":{"status":"error","timestamp":1579004294342,"user_tz":480,"elapsed":777269,"user":{"displayName":"Howard Goff","photoUrl":"","userId":"17409918526915385344"}}},"source":["# Make sure user does not accedentially execute beyond end\n","raise ExecutionStop(\"Stopping execution\")"],"execution_count":61,"outputs":[{"output_type":"error","ename":"ExecutionStop","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mExecutionStop\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-cced5245c4c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mExecutionStop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stopping execution\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mExecutionStop\u001b[0m: Stopping execution"]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lRKAhZK8yvO_"},"source":["# ====== Development Support Files (safe to ignore) ====="]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r_TVSYoeyvPa"},"source":["### SSH Setup\n","This is only neeeded if you want to log into the Colab machine. Otherwise fold it up and ignore.<br>\n","To use it you have to create a login at https://ngrok.com\n","<Details>Thanks to Imad El Hanafi (https://imadelhanafi.com) for showing me how to do this.<p>\n","You will need to create a free account at https://ngrok.com/ for the SSH tunnel to work.</Details>"]},{"cell_type":"markdown","metadata":{"id":"eK8m0r50aPHO","colab_type":"text"},"source":["File paths are hard coded here because this may be run before program variables are established."]},{"cell_type":"code","metadata":{"id":"36OI5-u2u24e","colab_type":"code","colab":{}},"source":["## if you want to use the Kaggle api from command line you will need a kaggle.json file\n","from pathlib import Path\n","if Path('/content/gdrive/My Drive/Colab/kaggle.json').exists() or \\\n","                                    Path('/content/kaggle.json').exists():\n","    pass    # we found a kaggle.json file\n","else:\n","    # Give user opportunity to upload a kaggle.json file\n","    from google.colab import files\n","    print('Upload kaggle.json if you want the Kaggle API to be availabel in bash.')\n","    # The files.upload() command is failing sporatically with:\n","    #   TypeError: Cannot read property '_uploadFiles' of undefined (just run this cell again)\n","    ! rm \"/content/kaggle.json\"  2> /dev/null\n","    files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y0NMPj1fyvPb","colab":{}},"source":["%%bash\n","## Install sshd; Set to allow login and config\n","apt-get install -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n","mkdir -p /var/run/sshd\n","echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n","echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\n","\n","# set host key to known value (need to test if exist)\n","gdown -O \"/etc/ssh/ssh_host_rsa_key\" --id 17Vp-rLM0kLVsIqxo7GkV3YXibGCJ7WCR\n","chown 600 \"/etc/ssh/ssh_host_rsa_key\"    # private key will be ignored if not secure\n","gdown -O \"/etc/ssh/ssh_host_rsa_key.pub\" --id 1-5yW1EwMdBN0YlRe7McmwDxzmGyvq-gW\n","# get script to modify login shell to match env of Notebook\n","gdown -O \"/root/init_shell.sh\" --id 1-9s5wuq5TkebgKbFvBYy4EeM8c2Ee0xc\n","\n","# this script will give fix the login shell so Python will work\n","if [ -f \"/root/init_shell.sh\" ]; then\n","    echo \"source /root/init_shell.sh\" >> /root/.bashrc\n","fi"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"S1mmJ__lyvPh","colab":{}},"source":["## setup ssh user / pass and start sshd\n","\n","#Generate a random root password\n","import random, string\n","sshpass = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(30))\n","\n","#Set root password\n","! echo root:$sshpass | chpasswd\n","\n","#Run sshd\n","get_ipython().system_raw('/usr/sbin/sshd -D &')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"edJ3pW6YyvPl","colab":{}},"source":["%%bash\n","## Get Ngrok from gdrive or try to download (see: https://ngrok.com/download)\n","if [ -f \"/content/bertqa/colab/ngrok-stable-linux-amd64.zip\" ]; then\n","    cp \"/content/bertqa/colab/ngrok-stable-linux-amd64.zip\" .\n","    echo \"Using ngrok-stable-linux-amd64.zip from gdrive\"\n","else\n","    wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","fi\n","unzip -qq -n ngrok-stable-linux-amd64.zip\n","rm ngrok-stable-linux-amd64.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YR0N4Iw8yvPq","colab":{}},"source":["## Get user to enter auth token from ngrok and start tunnel\n","\n","# Get token from ngrok for the tunnel\n","print(\"Get your authtoken from https://dashboard.ngrok.com/auth\")\n","import getpass\n","authtoken = getpass.getpass()\n","\n","#Create tunnel\n","get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1VjsRvCTyvPt"},"source":["#### ===============================<br> ||====&nbsp;&nbsp;  SSH Login Credentials &nbsp;&nbsp;====||<br> ==============================="]},{"cell_type":"code","metadata":{"colab_type":"code","cellView":"both","id":"WKjt0Wh0yvPv","colab":{}},"source":["#@title\n","print(\"username: root\")\n","print(\"password: \", sshpass)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7OlwbxpWyvPz"},"source":["Get the host name and port number at: https://dashboard.ngrok.com/status\n","\n","```bash\n","ssh root@0.tcp.ngrok.io -p [ngrok_port]\n","Login as: root\n","Servrer refused our key\n","root@0.tcp.ngrok.io's password: [see above]\n","\n","(Colab):/content$\n","```\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bjJcssgxyvP0"},"source":["Install programs"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"euzlUBHLyvP1","colab":{}},"source":["%%bash\n","# vim\n","apt-get install vim > /dev/null\n","echo \"set tabstop     =4\" >> ~/.vimrc\n","echo \"set softtabstop =4\" >> ~/.vimrc\n","echo \"set shiftwidth  =4\" >> ~/.vimrc\n","echo \"set expandtab\"      >> ~/.vimrc\n","\n","# js is a JSON processor\n","apt-get install js > /dev/null\n","\n","apt-get install tree > /dev/null\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ugtt5PxzyvP3"},"source":["If you need to kill Ngrok run this cell"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CiP9qYfgyvP4","colab":{}},"source":["if False:\n","    !kill $(ps aux | grep './ngrok' | awk '{print $2}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n3MP_pJ5yvP5"},"source":["## -- Misc Notes --"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cm6ErVGkyvP6"},"source":["### Prevent Disconnects\n","Colab periodically disconnects the browser.<br>\n","You have to save model checkpoints to Google Drive so you don't lose work<br>\n","See: https://mc.ai/google-colab-drive-as-persistent-storage-for-long-training-runs/<br>\n","Something to try...<br>\n","Ctrl+Shift+i in browser and in console run this code...\n","```\n","function KeepAlive(){\n","    console.log(\"Maintaining Connection\");\n","    document.querySelector(\"colab-toolbar-button#connect\").click()\n","}\n","setInterval(KeepAlive,60000);\n","```\n","There have been reports of people having their GPU privileges suspended for letting processes run for over 12 hours. It seems that they may penalize you rather than just cutting you off."]},{"cell_type":"markdown","metadata":{"id":"enrv0jdCyzCY","colab_type":"text"},"source":["### Monitor GPU\n","```\n","# From cli I think to monitor GPU while fiting\n","$ nvidia-smi dmon\n","$ nvidia-smi pmon\n","```"]},{"cell_type":"markdown","metadata":{"id":"j5B7tvshjsgs","colab_type":"text"},"source":["### Code From Elsewhere"]},{"cell_type":"code","metadata":{"id":"rbHIhd-POfaA","colab_type":"code","colab":{}},"source":["raise ExecutionStop(\"Stop Here\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gpTRsZHmzZcA","colab_type":"code","colab":{}},"source":["!nvidia-smi -i 0 -q -d MEMORY,UTILIZATION,POWER,CLOCK,COMPUTE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_yHN7478zx5s","colab_type":"code","colab":{}},"source":["%%bash\n","## Convert notebook to HTML or PDF for printing\n","\n","### Clear All Output & Save Before Doing This ###\n","\n","apt-get install texlive texlive-xetex texlive-latex-extra mandoc > /dev/null\n","pip install pypandoc\n","# jupyter nbconvert --to HTML /content/gdrive/My\\ Drive/Colab/bertqa/BERTjoint_yes_no/BERTjoint\\ yes\\ no2.ipynb\n","jupyter nbconvert --to HTML /content/gdrive/My\\ Drive/bertqa/BERTjoint_yes_no/BERTjoint\\ yes\\ no2.1.ipynb"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9AWB7ADoLGZR","colab_type":"code","colab":{}},"source":["# Do something with it\n","data_line = []\n","nrows = 10000\n","\n","# read the data\n","with open(f\"{train_file}\", 'rt') as f:\n","    for i in range(nrows):\n","        data_line.append(json.loads(f.readline()))\n"," \n","train_df = pd.DataFrame(data_line) # convert to data frame\n","train_df   # peek at data\n","\n","index = 0\n","train_df.loc[index, 'question_text']                # outputs the first question\n","\n","train_df.loc[index,'long_answer_candidates'][:5]    # outputs the first 5 \n","\n","train_df.loc[index,'annotations'][:5]               # outputs an array with answer information\n","\n","train_df.loc[index,'long_answer_candidates'][54]    # data for long answer\n","\n","' '.join(train_df.loc[index,'document_text'].split()[1952:2019])        # 1952 and 2019 are start/end tokens for long answer\n","\n","' '.join(train_df.loc[index,'document_text'].split()[1960:1969])        # 1960 and 1969 are start/end tokens for short answer\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vk7GFxxMNKej","colab_type":"text"},"source":["# ============ Notes / Eratta ============"]},{"cell_type":"markdown","metadata":{"id":"paCCac9Pa_bT","colab_type":"text"},"source":["###Things to possibly look at...<p>\n","TDS on Bert in Keras: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b<br>\n","HuggingFace Transformers: https://github.com/huggingface/transformers (includes tonenization code)<br>\n","cloud_tpu_custom_training (by TensorFlow): https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/custom_training.ipynb#scrollTo=FbVhjPpzn6BM<br>\n","Kaggle attempt to train with huggingface: https://www.kaggle.com/yihdarshieh/tf2-training-on-gcp-tpu/comments?scriptVersionId=26464952<br>\n","Comments on above: https://www.kaggle.com/c/tensorflow2-question-answering/discussion/124914<br>\n","BERT Fine Tuning w/TPU: https://github.com/tensorflow/models/blob/master/official/nlp/bert/bert_cloud_tpu.md\n"]}]}